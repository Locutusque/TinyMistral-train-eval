{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PyTorch Training Script with Hugging Face, Accelerate, and DeepSpeed\n\nThis notebook provides a parameterized training loop using PyTorch. It demonstrates how to load models and datasets from Hugging Face, distribute the model across multiple GPUs with Accelerate and DeepSpeed, and includes a custom dataset class that tokenizes examples simultaneously with the training loop.","metadata":{}},{"cell_type":"code","source":"# We'll need to install and setup accelerate and deepspeed configs for this notebook.\n# I'll also be installing one of my other packages for a model that I have private.\n# This package contains nice utilities that I'd rather not code again.\n\n!pip install --upgrade deepspeed accelerate sentia evaluate datasets transformers rouge_score\n!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n!python -m pip install -i https://pypi.anaconda.org/mpi4py/simple mpi4py\n!sudo apt install -y openmpi-bin\n!sudo apt install -y mpich\n!pip install --upgrade deepspeed accelerate sentia evaluate datasets transformers rouge_score\n!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n!python -m pip install -i https://pypi.anaconda.org/mpi4py/simple mpi4py","metadata":{"execution":{"iopub.status.busy":"2023-12-08T01:12:47.706840Z","iopub.execute_input":"2023-12-08T01:12:47.707126Z","iopub.status.idle":"2023-12-08T01:15:06.077018Z","shell.execute_reply.started":"2023-12-08T01:12:47.707100Z","shell.execute_reply":"2023-12-08T01:15:06.075673Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting deepspeed\n  Downloading deepspeed-0.12.4.tar.gz (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.24.1)\nCollecting accelerate\n  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/f7/fc/c55e5a2da345c9a24aa2e1e0f60eb2ca290b6a41be82da03a6d4baec4f99/accelerate-0.25.0-py3-none-any.whl.metadata\n  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\nCollecting sentia\n  Obtaining dependency information for sentia from https://files.pythonhosted.org/packages/25/f5/0849c01280d703493ec2b2b01e60aa0b2eae9592c979f75dc0f341bf1755/sentia-1.17-py3-none-any.whl.metadata\n  Downloading sentia-1.17-py3-none-any.whl.metadata (1.9 kB)\nCollecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.0)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting hjson (from deepspeed)\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.12)\nRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from deepspeed) (11.4.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.66.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.17.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sentia) (0.4.6)\nCollecting configparser (from sentia)\n  Obtaining dependency information for configparser from https://files.pythonhosted.org/packages/81/a3/0e5ed11da4b7770c15f6f319abf053f46b5a06c7d4273c48469b7899bd89/configparser-6.0.0-py3-none-any.whl.metadata\n  Downloading configparser-6.0.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentia) (3.2.4)\nCollecting rotary-embedding-torch (from sentia)\n  Obtaining dependency information for rotary-embedding-torch from https://files.pythonhosted.org/packages/4b/a2/039415e2c340e0bbcea65beaa3ab23332bab9493d0d73d76f2c5793d2bcc/rotary_embedding_torch-0.4.0-py3-none-any.whl.metadata\n  Downloading rotary_embedding_torch-0.4.0-py3-none-any.whl.metadata (702 bytes)\nCollecting sacrebleu (from sentia)\n  Obtaining dependency information for sacrebleu from https://files.pythonhosted.org/packages/0a/a6/2ac47e71e526bbcd97ea08f20d9ef7d3852e2594ec7b2d55f5d2bbfd7aae/sacrebleu-2.3.3-py3-none-any.whl.metadata\n  Downloading sacrebleu-2.3.3-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from sentia) (0.16.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.10.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nCollecting pyarrow-hotfix (from datasets)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nCollecting huggingface-hub (from accelerate)\n  Obtaining dependency information for huggingface-hub from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->deepspeed) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nINFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\nCollecting tokenizers<0.19,>=0.14 (from transformers)\n  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/eb/3d/eee5f3c572a3f4db2ebabf5bd4f284f356078a5b5d27e6229b4450d5c5e4/tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nCollecting beartype (from rotary-embedding-torch->sentia)\n  Obtaining dependency information for beartype from https://files.pythonhosted.org/packages/46/8a/a90fe78c73958340ed6b6ab128a10598ad5f0ff57537ad17f6ccd1ad830b/beartype-0.16.4-py3-none-any.whl.metadata\n  Downloading beartype-0.16.4-py3-none-any.whl.metadata (29 kB)\nCollecting einops>=0.7 (from rotary-embedding-torch->sentia)\n  Obtaining dependency information for einops>=0.7 from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nCollecting portalocker (from sacrebleu->sentia)\n  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->sentia) (0.9.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->sentia) (4.9.3)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (3.1.32)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (1.34.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (0.4.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (3.20.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->sentia) (4.0.10)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->sentia) (5.0.0)\nDownloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentia-1.17-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading configparser-6.0.0-py3-none-any.whl (19 kB)\nDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading rotary_embedding_torch-0.4.0-py3-none-any.whl (5.1 kB)\nDownloading sacrebleu-2.3.3-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.4/106.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading beartype-0.16.4-py3-none-any.whl (819 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.1/819.1 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: deepspeed, rouge_score\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.12.4-py3-none-any.whl size=1290641 sha256=3c96099d1fc5b6badd32c6380ad42064ab90b8115af1fbb2f569ba6048830745\n  Stored in directory: /root/.cache/pip/wheels/7b/0c/52/f464610477b069120f740202a9d84a27f9d7235cbf035c4b75\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=63f2bf38453641d04d713bc2b8e08a12e177da942c5960aa0f1e2801cde1da82\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built deepspeed rouge_score\nInstalling collected packages: hjson, pyarrow-hotfix, portalocker, einops, configparser, beartype, sacrebleu, rouge_score, huggingface-hub, tokenizers, rotary-embedding-torch, deepspeed, accelerate, transformers, datasets, sentia, evaluate\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.17.3\n    Uninstalling huggingface-hub-0.17.3:\n      Successfully uninstalled huggingface-hub-0.17.3\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.14.1\n    Uninstalling tokenizers-0.14.1:\n      Successfully uninstalled tokenizers-0.14.1\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.24.1\n    Uninstalling accelerate-0.24.1:\n      Successfully uninstalled accelerate-0.24.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.35.0\n    Uninstalling transformers-4.35.0:\n      Successfully uninstalled transformers-4.35.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed accelerate-0.25.0 beartype-0.16.4 configparser-6.0.0 datasets-2.15.0 deepspeed-0.12.4 einops-0.7.0 evaluate-0.4.1 hjson-3.1.0 huggingface-hub-0.19.4 portalocker-2.8.2 pyarrow-hotfix-0.6 rotary-embedding-torch-0.4.0 rouge_score-0.1.2 sacrebleu-2.3.3 sentia-1.17 tokenizers-0.15.0 transformers-4.35.2\nLooking in indexes: https://download.pytorch.org/whl/nightly/cu121\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nLooking in indexes: https://pypi.anaconda.org/mpi4py/simple\nCollecting mpi4py\n  Downloading https://pypi.anaconda.org/mpi4py/simple/mpi4py/3.1.5/mpi4py-3.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: mpi4py\nSuccessfully installed mpi4py-3.1.5\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  ibverbs-providers libevent-pthreads-2.1-7 libfabric1 libhwloc-plugins\n  libhwloc15 libopenmpi3 libpmix2 libpsm-infinipath1 libpsm2-2 librdmacm1\n  libucx0 libxnvctrl0 openmpi-common\nSuggested packages:\n  libhwloc-contrib-plugins gfortran | fortran-compiler\nThe following NEW packages will be installed:\n  ibverbs-providers libevent-pthreads-2.1-7 libfabric1 libhwloc-plugins\n  libhwloc15 libopenmpi3 libpmix2 libpsm-infinipath1 libpsm2-2 librdmacm1\n  libucx0 libxnvctrl0 openmpi-bin openmpi-common\n0 upgraded, 14 newly installed, 0 to remove and 46 not upgraded.\nNeed to get 5893 kB of archives.\nAfter this operation, 19.8 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 ibverbs-providers amd64 39.0-1 [341 kB]\nGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libxnvctrl0 545.23.06-0ubuntu1 [21.3 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-pthreads-2.1-7 amd64 2.1.12-stable-1build3 [7642 B]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpsm-infinipath1 amd64 3.3+20.604758e7-6.1 [170 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpsm2-2 amd64 11.2.185-1 [182 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 librdmacm1 amd64 39.0-1 [71.2 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfabric1 amd64 1.11.0-3 [558 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libhwloc15 amd64 2.7.0-2ubuntu1 [159 kB]\nGet:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libhwloc-plugins amd64 2.7.0-2ubuntu1 [15.6 kB]\nGet:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpmix2 amd64 4.1.2-2ubuntu1 [604 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libucx0 amd64 1.12.1~rc2-1 [891 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenmpi3 amd64 4.1.2-2ubuntu1 [2594 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 openmpi-common all 4.1.2-2ubuntu1 [162 kB]\nGet:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 openmpi-bin amd64 4.1.2-2ubuntu1 [116 kB]\nFetched 5893 kB in 1s (11.2 MB/s)      \u001b[0m\u001b[33m\n\n\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package ibverbs-providers:amd64.\n(Reading database ... 114840 files and directories currently installed.)\nPreparing to unpack .../00-ibverbs-providers_39.0-1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking ibverbs-providers:amd64 (39.0-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Selecting previously unselected package libevent-pthreads-2.1-7:amd64.\nPreparing to unpack .../01-libevent-pthreads-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libevent-pthreads-2.1-7:amd64 (2.1.12-stable-1build3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package libpsm-infinipath1.\nPreparing to unpack .../02-libpsm-infinipath1_3.3+20.604758e7-6.1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking libpsm-infinipath1 (3.3+20.604758e7-6.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package libpsm2-2.\nPreparing to unpack .../03-libpsm2-2_11.2.185-1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libpsm2-2 (11.2.185-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package librdmacm1:amd64.\nPreparing to unpack .../04-librdmacm1_39.0-1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking librdmacm1:amd64 (39.0-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libfabric1:amd64.\nPreparing to unpack .../05-libfabric1_1.11.0-3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking libfabric1:amd64 (1.11.0-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libhwloc15:amd64.\nPreparing to unpack .../06-libhwloc15_2.7.0-2ubuntu1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libhwloc15:amd64 (2.7.0-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Selecting previously unselected package libxnvctrl0:amd64.\nPreparing to unpack .../07-libxnvctrl0_545.23.06-0ubuntu1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking libxnvctrl0:amd64 (545.23.06-0ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package libhwloc-plugins:amd64.\nPreparing to unpack .../08-libhwloc-plugins_2.7.0-2ubuntu1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking libhwloc-plugins:amd64 (2.7.0-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libpmix2:amd64.\nPreparing to unpack .../09-libpmix2_4.1.2-2ubuntu1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libpmix2:amd64 (4.1.2-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package libucx0:amd64.\nPreparing to unpack .../10-libucx0_1.12.1~rc2-1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Unpacking libucx0:amd64 (1.12.1~rc2-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package libopenmpi3:amd64.\nPreparing to unpack .../11-libopenmpi3_4.1.2-2ubuntu1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Unpacking libopenmpi3:amd64 (4.1.2-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package openmpi-common.\nPreparing to unpack .../12-openmpi-common_4.1.2-2ubuntu1_all.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking openmpi-common (4.1.2-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Selecting previously unselected package openmpi-bin.\nPreparing to unpack .../13-openmpi-bin_4.1.2-2ubuntu1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Unpacking openmpi-bin (4.1.2-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Setting up librdmacm1:amd64 (39.0-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up ibverbs-providers:amd64 (39.0-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libxnvctrl0:amd64 (545.23.06-0ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libhwloc15:amd64 (2.7.0-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libpsm2-2 (11.2.185-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up openmpi-common (4.1.2-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libpsm-infinipath1 (3.3+20.604758e7-6.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [#########################################.................] \u001b8update-alternatives: using /usr/lib/libpsm1/libpsm_infinipath.so.1.16 to provide /usr/lib/x86_64-linux-gnu/libpsm_infinipath.so.1 (libpsm_infinipath.so.1) in auto mode\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libevent-pthreads-2.1-7:amd64 (2.1.12-stable-1build3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libfabric1:amd64 (1.11.0-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libucx0:amd64 (1.12.1~rc2-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libhwloc-plugins:amd64 (2.7.0-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libpmix2:amd64 (4.1.2-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libopenmpi3:amd64 (4.1.2-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up openmpi-bin (4.1.2-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8update-alternatives: using /usr/bin/mpirun.openmpi to provide /usr/bin/mpirun (mpirun) in auto mode\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpirun.1.gz because associated file /usr/share/man/man1/mpirun.openmpi.1.gz (of link group mpirun) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpiexec.1.gz because associated file /usr/share/man/man1/mpiexec.openmpi.1.gz (of link group mpirun) doesn't exist\nupdate-alternatives: using /usr/bin/mpicc.openmpi to provide /usr/bin/mpicc (mpi) in auto mode\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpicc.1.gz because associated file /usr/share/man/man1/mpicc.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpic++.1.gz because associated file /usr/share/man/man1/mpic++.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpicxx.1.gz because associated file /usr/share/man/man1/mpicxx.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpiCC.1.gz because associated file /usr/share/man/man1/mpiCC.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpif77.1.gz because associated file /usr/share/man/man1/mpif77.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpif90.1.gz because associated file /usr/share/man/man1/mpif90.openmpi.1.gz (of link group mpi) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/mpifort.1.gz because associated file /usr/share/man/man1/mpifort.openmpi.1.gz (of link group mpi) doesn't exist\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.1) ...\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  gfortran gfortran-11 hwloc-nox libgfortran-11-dev libmpich-dev libmpich12\n  libslurm37\nSuggested packages:\n  gfortran-multilib gfortran-doc gfortran-11-multilib gfortran-11-doc\n  libcoarrays-dev mpich-doc\nThe following NEW packages will be installed:\n  gfortran gfortran-11 hwloc-nox libgfortran-11-dev libmpich-dev libmpich12\n  libslurm37 mpich\n0 upgraded, 8 newly installed, 0 to remove and 46 not upgraded.\nNeed to get 26.2 MB of archives.\nAfter this operation, 137 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgfortran-11-dev amd64 11.4.0-1ubuntu1~22.04 [842 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gfortran-11 amd64 11.4.0-1ubuntu1~22.04 [11.2 MB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 gfortran amd64 4:11.2.0-1ubuntu1 [1182 B]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libslurm37 amd64 21.08.5-2ubuntu1 [542 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 hwloc-nox amd64 2.7.0-2ubuntu1 [205 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmpich12 amd64 4.0-3 [5866 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 mpich amd64 4.0-3 [197 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmpich-dev amd64 4.0-3 [7375 kB]\nFetched 26.2 MB in 1s (28.9 MB/s)      \u001b[0m\u001b[33m\n\n\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libgfortran-11-dev:amd64.\n(Reading database ... 115378 files and directories currently installed.)\nPreparing to unpack .../0-libgfortran-11-dev_11.4.0-1ubuntu1~22.04_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  3%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking libgfortran-11-dev:amd64 (11.4.0-1ubuntu1~22.04) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Selecting previously unselected package gfortran-11.\nPreparing to unpack .../1-gfortran-11_11.4.0-1ubuntu1~22.04_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking gfortran-11 (11.4.0-1ubuntu1~22.04) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Selecting previously unselected package gfortran.\nPreparing to unpack .../2-gfortran_4%3a11.2.0-1ubuntu1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Unpacking gfortran (4:11.2.0-1ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libslurm37.\nPreparing to unpack .../3-libslurm37_21.08.5-2ubuntu1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Unpacking libslurm37 (21.08.5-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [##############............................................] \u001b8Selecting previously unselected package hwloc-nox.\nPreparing to unpack .../4-hwloc-nox_2.7.0-2ubuntu1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking hwloc-nox (2.7.0-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package libmpich12:amd64.\nPreparing to unpack .../5-libmpich12_4.0-3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libmpich12:amd64 (4.0-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package mpich.\nPreparing to unpack .../6-mpich_4.0-3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking mpich (4.0-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package libmpich-dev:amd64.\nPreparing to unpack .../7-libmpich-dev_4.0-3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 45%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking libmpich-dev:amd64 (4.0-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [############################..............................] \u001b8Setting up libslurm37 (21.08.5-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libgfortran-11-dev:amd64 (11.4.0-1ubuntu1~22.04) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up hwloc-nox (2.7.0-2ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up gfortran-11 (11.4.0-1ubuntu1~22.04) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libmpich12:amd64 (4.0-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up mpich (4.0-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up gfortran (4:11.2.0-1ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [##################################################........] \u001b8update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f95 (f95) in auto mode\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/f95.1.gz because associated file /usr/share/man/man1/gfortran.1.gz (of link group f95) doesn't exist\nupdate-alternatives: using /usr/bin/gfortran to provide /usr/bin/f77 (f77) in auto mode\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/f77.1.gz because associated file /usr/share/man/man1/gfortran.1.gz (of link group f77) doesn't exist\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libmpich-dev:amd64 (4.0-3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8update-alternatives: using /usr/include/x86_64-linux-gnu/mpich to provide /usr/include/x86_64-linux-gnu/mpi (mpi-x86_64-linux-gnu) in auto mode\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 97%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.1) ...\n\n\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JRequirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (0.12.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nRequirement already satisfied: sentia in /opt/conda/lib/python3.10/site-packages (1.17)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.2)\nRequirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed) (3.1.0)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.12)\nRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from deepspeed) (11.4.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.66.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.19.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sentia) (0.4.6)\nRequirement already satisfied: configparser in /opt/conda/lib/python3.10/site-packages (from sentia) (6.0.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentia) (3.2.4)\nRequirement already satisfied: rotary-embedding-torch in /opt/conda/lib/python3.10/site-packages (from sentia) (0.4.0)\nRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (from sentia) (2.3.3)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from sentia) (0.16.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.10.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->deepspeed) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: beartype in /opt/conda/lib/python3.10/site-packages (from rotary-embedding-torch->sentia) (0.16.4)\nRequirement already satisfied: einops>=0.7 in /opt/conda/lib/python3.10/site-packages (from rotary-embedding-torch->sentia) (0.7.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu->sentia) (2.8.2)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->sentia) (0.9.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->sentia) (4.9.3)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (3.1.32)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (1.34.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (0.4.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (3.20.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->sentia) (4.0.10)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->sentia) (5.0.0)\nLooking in indexes: https://download.pytorch.org/whl/nightly/cu121\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nLooking in indexes: https://pypi.anaconda.org/mpi4py/simple\nRequirement already satisfied: mpi4py in /opt/conda/lib/python3.10/site-packages (3.1.5)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary modules\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nfrom torch.optim import AdamW\nimport torch.nn.functional as F\nfrom sentia import SENTIAForCausalLM # This the model I was talking about,\n# it includes methods for accuracy calculation\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom accelerate import Accelerator, notebook_launcher\nimport accelerate\nimport wandb\nfrom tqdm import tqdm\nimport sacrebleu\nfrom datasets import load_dataset\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Tuple\nfrom evaluate import load as load_metric\nimport warnings","metadata":{"execution":{"iopub.status.busy":"2023-12-08T01:15:06.079344Z","iopub.execute_input":"2023-12-08T01:15:06.079700Z","iopub.status.idle":"2023-12-08T01:15:31.503502Z","shell.execute_reply.started":"2023-12-08T01:15:06.079668Z","shell.execute_reply":"2023-12-08T01:15:31.502721Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Custom Dataset Classes\n\nThese classes are designed to tokenize data on-the-fly, which can be more memory-efficient for large datasets.","metadata":{}},{"cell_type":"code","source":"class ConversationDataset(Dataset):\n    def __init__(self, tokenizer, max_length=512, data=None, device=\"cuda\"):\n        self.data = data\n        self.device = device\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        try:\n            # Most of the time I'll be using InstructMix for instruction-tuning\n            user = self.data[idx][\"Input\"]\n            assistant = self.data[idx][\"Output\"]\n        except KeyError:\n            # If I'm using MMLU for evaluation\n            user = self.data[idx][\"question\"]\n            ans_index = self.data[idx][\"answer\"]\n            assistant = self.data[idx][\"choices\"][ans_index]\n        \n        input_text = f\"<|ASSISTANT|> {assistant} <|USER|> {user} {self.tokenizer.pad_token}\"\n        target_text = f\"<|ASSISTANT|> {assistant} <|USER|> {user} {self.tokenizer.pad_token}\"\n        input_ids = self.tokenizer.encode(input_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        target_ids = self.tokenizer.encode(target_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n        target_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(target_ids))\n\n        return {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.int64, device=self.device),\n            \"labels\": torch.tensor(target_ids, dtype=torch.int64, device=self.device),\n        }\nclass CompletionDataset(Dataset):\n    def __init__(self, tokenizer, data, max_length=256, device=\"cuda\"):\n        self.data = data\n        self.device = device\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data[idx][\"text\"]\n        input_text = f\"{text} {self.tokenizer.eos_token}\"\n        target_text = f\"{text} {self.tokenizer.eos_token}\"\n        input_ids = self.tokenizer.encode(input_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        target_ids = self.tokenizer.encode(target_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n        target_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(target_ids))\n        return {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.int64, device=self.device),\n            \"labels\": torch.tensor(target_ids, dtype=torch.int64, device=self.device),\n        }","metadata":{"execution":{"iopub.status.busy":"2023-12-08T01:43:45.652842Z","iopub.execute_input":"2023-12-08T01:43:45.653237Z","iopub.status.idle":"2023-12-08T01:43:45.669308Z","shell.execute_reply.started":"2023-12-08T01:43:45.653203Z","shell.execute_reply":"2023-12-08T01:43:45.668424Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Training args\nWe'll define some training arguments, these will be able to control the parameters of training.\n","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass TrainingArguments:\n    # Model and tokenizer arguments\n    model_name_or_path: str\n    tokenizer_name: Optional[str] = None\n    \n    # Training data arguments\n    train_data_file: str = None\n    eval_data_file: str = None\n    train_data_config: Optional[str] = None\n    eval_data_config: Optional[str] = None\n    max_seq_length: int = 512\n    \n    # Training procedure arguments\n    num_train_epochs: int = 3\n    train_batch_size: int = 8\n    eval_batch_size: int = 8\n    learning_rate: float = 5e-5\n    weight_decay: float = 0.01\n    adam_epsilon: float = 1e-8\n    max_grad_norm: float = 1.0\n    gradient_accumulation_steps: int = 1\n    \n    # Logging, saving, and evaluation arguments\n    print_predictions: bool = False\n    logging_steps: int = 50\n    output_dir: str = \"./output\"\n    \n    # DeepSpeed configuration\n    deepspeed_config_file: Tuple[dict, str] = None\n    \n    # Accelerate configuration\n    mixed_precision: str = \"no\"  # Options: \"no\", \"fp16\", \"bf16\"\n    \n    # WandB configuration\n    use_wandb: bool = False\n    wandb_project: Optional[str] = None\n    wandb_entity: Optional[str] = None\n    \n    # Other arguments\n    seed: int = 42\n    device: str = \"cuda\"  # Options: \"cuda\", \"cpu\"\n    local_rank: int = -1  # For distributed training: local_rank for distributed training on gpus\n\n    def __post_init__(self):\n        if self.tokenizer_name is None:\n            self.tokenizer_name = self.model_name_or_path\n        if self.local_rank == -1:\n            # If not using distributed training, set mixed precision\n            if self.mixed_precision == \"fp16\":\n                self.fp16 = True\n            elif self.mixed_precision == \"bf16\":\n                self.bf16 = True\n            else:\n                self.fp16 = False\n                self.bf16 = False\n        else:\n            # For distributed training, disable mixed precision here\n            # and let deepspeed handle it\n            self.fp16 = False\n            self.bf16 = False\n        if self.use_wandb and (self.wandb_project is None):\n            raise ValueError(\"wandb project name must be defined if use_wandb = True\")","metadata":{"execution":{"iopub.status.busy":"2023-12-08T01:15:31.524207Z","iopub.execute_input":"2023-12-08T01:15:31.524523Z","iopub.status.idle":"2023-12-08T01:15:31.542237Z","shell.execute_reply.started":"2023-12-08T01:15:31.524498Z","shell.execute_reply":"2023-12-08T01:15:31.541520Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Training and Evaluation Functions\n\nThese functions define the training and evaluation loops for the model. They use the `Accelerator` class for GPU acceleration.","metadata":{}},{"cell_type":"code","source":"def train(model, dataloader, optimizer, tokenizer, args: TrainingArguments, accelerator: Accelerator):\n    model.train()\n    total_loss = 0\n    total_perplexity = 0\n    global_step = 0\n\n    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n        with accelerator.accumulate():\n            # Move batch to the correct device\n            batch = {k: v.to(args.device) for k, v in batch.items()}\n            input_ids = batch[\"input_ids\"]\n            labels = batch[\"labels\"]\n\n            # Generate the output and calculate the loss\n            outputs = model(input_ids=input_ids, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n            # Backward pass\n            accelerator.backward(loss)\n            if args.max_grad_norm is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n            # Update model parameters\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Calculate the BLEU score and accuracy\n            predictions = torch.argmax(logits, dim=-1)\n            predictions_str = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions.tolist()]\n            target_ids_str = [tokenizer.decode(tgt, skip_special_tokens=True) for tgt in batch[\"labels\"].tolist()]\n            print(predictions_str[0])\n            bleu_scores = []\n            accuracy_scores = []\n            for pred_str, target_str in zip(predictions_str, target_ids_str):\n                bleu = sacrebleu.sentence_bleu(pred_str, [target_str])\n                bleu_scores.append(bleu.score)\n\n            bleu = sum(bleu_scores) / len(bleu_scores)\n\n            # Logging\n            try:\n                wandb.log({\n                        \"loss\": loss.item(),\n                        \"bleu\": bleu,\n                        \"perplexity\": torch.exp(loss).item(),\n                    })\n            except Exception as e:\n                warnings.warn(f\"An error occurred while logging to Weights & Biases: {e}\")\n\n            # Print training information\n            if global_step % args.logging_steps == 0:\n                print(f\"Step {global_step}: Loss: {loss.item():.4f}, BLEU: {bleu:.4f}, Perplexity: {torch.exp(loss).item():.4f}\")\n            \n\n            # Update the metrics\n            total_loss += loss.item()\n            total_perplexity += torch.exp(loss).item()\n\n    return total_loss / len(dataloader), total_perplexity / len(dataloader)\n\ndef evaluate(model, val_loader, tokenizer, use_cuda=True):\n    model.eval()\n    device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Load metrics\n    bleu_metric = load_metric('bleu')\n    rouge_metric = load_metric('rouge')\n    \n    # Initialize variables to accumulate scores\n    total_loss = 0\n    all_predictions = []\n    all_references = []\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n            # Move batch to the correct device\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            \n            # Forward pass\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            # Convert logits to predictions (for F1, BLEU, ROUGE)\n            # This part depends on your model's output format and the task\n            # Here is a mock-up of how you might extract predictions\n            # For token classification tasks:\n            # predictions = outputs.logits.argmax(dim=-1)\n            # For seq2seq tasks:\n            predictions = tokenizer.batch_decode(outputs.logits.argmax(dim=-1), skip_special_tokens=True)\n\n            # Post-process batch to extract labels and predictions in a suitable format\n            references = batch['labels'] \n            references = tokenizer.batch_decode(references, skip_special_tokens=True)\n            \n            # Update metrics\n            references = [[ref] for ref in references]\n            bleu_metric.add_batch(predictions=predictions, references=references)\n            rouge_metric.add_batch(predictions=predictions, references=references)\n            # Store predictions and references for later use if needed\n            all_predictions.extend(predictions)\n            all_references.extend(references)\n    # Compute the metrics\n    bleu_score = bleu_metric.compute(predictions=all_predictions, references=all_references)\n    rouge_score = rouge_metric.compute(predictions=all_predictions, references=all_references)\n\n    # Perplexity can be calculated from the total loss\n    # For perplexity, we assume the loss is the negative log likelihood\n    # In case the loss function is something else, this needs to be adjusted\n    perplexity = torch.exp(torch.tensor(total_loss / len(val_loader)))\n\n    metrics = {\n        'val_loss': total_loss / len(val_loader),\n        'val_perplexity': perplexity.item(),\n        'val_bleu': bleu_score['bleu'],\n        'val_rouge': rouge_score,\n    }\n    try:\n        wandb.log(**metrics)\n    except:\n        pass\n\n    return metrics\n\n        ","metadata":{"execution":{"iopub.status.busy":"2023-12-08T01:15:31.543483Z","iopub.execute_input":"2023-12-08T01:15:31.543759Z","iopub.status.idle":"2023-12-08T01:15:31.850041Z","shell.execute_reply.started":"2023-12-08T01:15:31.543735Z","shell.execute_reply":"2023-12-08T01:15:31.849030Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop\nHere, we'll load the model, datasets, and tokenizer and start the training loop.","metadata":{}},{"cell_type":"code","source":"def main():\n    !export CUDA_VISIBLE_DEVICES=0,1\n    deepspeed_config_dict = {\n        \"train_micro_batch_size_per_gpu\": 4,\n        \"gradient_accumulation_steps\": 1,\n        \"fp16\": {\n            \"enabled\": True,\n            \"loss_scale\": 0,\n            \"loss_scale_window\": 1000,\n            \"min_loss_scale\": 1,\n            \"hysteresis\": 2\n        },\n        \"zero_optimization\": {\n            \"stage\": 2,\n            \"offload_optimizer\": {\n                \"device\": \"cpu\",\n                \"pin_memory\": True\n            },\n            \"allgather_partitions\": True,\n            \"allgather_bucket_size\": 2e8,\n            \"overlap_comm\": True,\n            \"reduce_scatter\": True,\n            \"reduce_bucket_size\": 2e8,\n            \"contiguous_gradients\": True\n        },\n        \"activation_checkpointing\": {\n            \"partition_activations\": True,\n            \"cpu_checkpointing\": False,\n            \"contiguous_memory_optimization\": False,\n            \"synchronize_checkpoint_boundary\": False\n        },\n        \"steps_per_print\": 1,\n        \"wall_clock_breakdown\": False\n    }\n    deepspeed_config_dict = accelerate.utils.DeepSpeedPlugin(hf_ds_config=deepspeed_config_dict)\n    args = TrainingArguments(\n        model_name_or_path=\"Locutusque/TinyMistral-248M\",\n        train_data_file=\"Locutusque/InstructMix-V2\",\n        eval_data_file=\"cais/mmlu\",\n        eval_data_config=\"all\",\n        max_seq_length=256,\n        max_grad_norm=2.0,\n        train_batch_size=4,\n        eval_batch_size=4,\n        gradient_accumulation_steps=1,\n        adam_epsilon=1e-4,\n        use_wandb=False,\n        print_predictions=False,\n        deepspeed_config_file=deepspeed_config_dict,\n        num_train_epochs=1\n\n    )\n    use_wandb = args.use_wandb\n    # Initialize Weights & Biases if you're using it\n    if use_wandb:\n        wandb.init(project=args.wandb_project, entity=args.wandb_entity, settings=wandb.Settings(start_method=\"fork\"))\n\n    # Initialize the Accelerator and tokenizer\n    print(\"Installing the tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name if args.tokenizer_name is not None else args.model_name_or_path)\n    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|USER|>\", \"<|ASSISTANT|>\"]})\n    tokenizer.pad_token = tokenizer.eos_token\n    print(\"Initializing the accelerator\")\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, deepspeed_plugin=args.deepspeed_config_file)\n    print(accelerator.state.num_processes)\n\n    # Prepare the dataset and dataloader\n    print(\"Installing the datasets\")\n    train_data = load_dataset(args.train_data_file, args.train_data_config, split=\"train[:100]\")\n    val_data = load_dataset(args.eval_data_file, args.eval_data_config, split=\"validation\")\n    train_dataset = ConversationDataset(tokenizer=tokenizer, data=train_data)\n    val_dataset = ConversationDataset(tokenizer=tokenizer, data=val_data)\n    train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=args.eval_batch_size)\n\n    # Prepare the model and optimizer\n    print(\"Installing the model\")\n    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, torch_dtype=torch.float16).to(\"cuda:0\")\n    model.resize_token_embeddings()\n    optimizer = AdamW(model.parameters(), lr=args.learning_rate, fused=True, eps=args.adam_epsilon)\n\n    # Prepare the model, optimizer, and dataloaders for distributed training\n    model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, val_dataloader\n    )\n\n    # Training loop\n    num_epochs = args.num_train_epochs\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        train_loss = train(model, train_dataloader, optimizer, tokenizer, args=args, accelerator=accelerator)\n        metrics = evaluate(model, val_dataloader, tokenizer, use_cuda=True)\n        print(f\"Training Loss: {train_loss}\")\n        print(f\"Validation Metrics: {metrics}\")\n    accelerator.wait_for_everyone()\n    if args.output_dir is not None and accelerator.is_main_process:\n        unwrapped_model = accelerator.save_model(model, \"/kaggle/working/\")\n        tokenizer.save_pretrained(\"/kaggle/working/\")\n\n    # Finalize Weights & Biases run\n    if use_wandb:\n        wandb.finish(quiet=True)\n\n    print(\"Training complete!\")\nif __name__ == \"__main__\":\n    notebook_launcher(\n    main,\n    num_processes=2\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:05:18.274861Z","iopub.execute_input":"2023-12-08T02:05:18.275719Z","iopub.status.idle":"2023-12-08T02:07:09.552242Z","shell.execute_reply.started":"2023-12-08T02:05:18.275678Z","shell.execute_reply":"2023-12-08T02:07:09.550880Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Launching training on 2 GPUs.\nInstalling the tokenizer\nInstalling the tokenizer\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Initializing the accelerator\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Initializing the accelerator\n[2023-12-08 02:05:19,714] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-12-08 02:05:19,760] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-12-08 02:05:28,217] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-12-08 02:05:28,238] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-12-08 02:05:28,239] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n22\n\nInstalling the datasetsInstalling the datasets\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72222c5932f8432788993455daf3411f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d44d4a5ce66d40e6b14f817808e9a492"}},"metadata":{}},{"name":"stdout","text":"Installing the model\nInstalling the model\n","output_type":"stream"},{"name":"stderr","text":"Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\nBuilding extension module cpu_adam...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module cpu_adam...\n","output_type":"stream"},{"name":"stdout","text":"Time to load cpu_adam op: 2.734468936920166 seconds\n","output_type":"stream"},{"name":"stderr","text":"Loading extension module cpu_adam...\n","output_type":"stream"},{"name":"stdout","text":"Time to load cpu_adam op: 2.7897238731384277 seconds\nninja: no work to do.\n[2023-12-08 02:05:48,642] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\nConfig: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n[2023-12-08 02:05:50,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n[2023-12-08 02:05:50,987] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n[2023-12-08 02:05:50,989] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n[2023-12-08 02:05:50,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n[2023-12-08 02:05:50,996] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n[2023-12-08 02:05:50,998] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n[2023-12-08 02:05:51,000] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n[2023-12-08 02:05:51,002] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n[2023-12-08 02:05:51,004] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n[2023-12-08 02:05:51,007] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\nAdam Optimizer #0 is created with AVX512 arithmetic capability.\nConfig: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n[2023-12-08 02:05:53,794] [INFO] [utils.py:795:see_memory_usage] Before initializing optimizer states\n[2023-12-08 02:05:53,797] [INFO] [utils.py:796:see_memory_usage] MA 0.57 GB         Max_MA 0.57 GB         CA 0.72 GB         Max_CA 1 GB \n[2023-12-08 02:05:53,799] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 12.86 GB, percent = 41.0%\nEpoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/13 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[2023-12-08 02:05:55,535] [INFO] [utils.py:795:see_memory_usage] After initializing optimizer states\n[2023-12-08 02:05:55,538] [INFO] [utils.py:796:see_memory_usage] MA 0.57 GB         Max_MA 0.57 GB         CA 0.72 GB         Max_CA 1 GB \n[2023-12-08 02:05:55,541] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 14.38 GB, percent = 45.9%\n[2023-12-08 02:05:55,542] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n[2023-12-08 02:05:56,158] [INFO] [utils.py:795:see_memory_usage] After initializing ZeRO optimizer\n[2023-12-08 02:05:56,164] [INFO] [utils.py:796:see_memory_usage] MA 0.57 GB         Max_MA 0.57 GB         CA 0.72 GB         Max_CA 1 GB \n[2023-12-08 02:05:56,166] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 14.4 GB, percent = 45.9%\n[2023-12-08 02:05:56,317] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam\n[2023-12-08 02:05:56,320] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n[2023-12-08 02:05:56,327] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n[2023-12-08 02:05:56,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]\n[2023-12-08 02:05:56,339] [INFO] [config.py:979:print] DeepSpeedEngine configuration:\n[2023-12-08 02:05:56,352] [INFO] [config.py:983:print]   activation_checkpointing_config  {\n    \"partition_activations\": true, \n    \"contiguous_memory_optimization\": false, \n    \"cpu_checkpointing\": false, \n    \"number_checkpoints\": null, \n    \"synchronize_checkpoint_boundary\": false, \n    \"profile\": false\n}\n[2023-12-08 02:05:56,356] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n[2023-12-08 02:05:56,358] [INFO] [config.py:983:print]   amp_enabled .................. False\n[2023-12-08 02:05:56,361] [INFO] [config.py:983:print]   amp_params ................... False\n[2023-12-08 02:05:56,365] [INFO] [config.py:983:print]   autotuning_config ............ {\n    \"enabled\": false, \n    \"start_step\": null, \n    \"end_step\": null, \n    \"metric_path\": null, \n    \"arg_mappings\": null, \n    \"metric\": \"throughput\", \n    \"model_info\": null, \n    \"results_dir\": \"autotuning_results\", \n    \"exps_dir\": \"autotuning_exps\", \n    \"overwrite\": true, \n    \"fast\": true, \n    \"start_profile_step\": 3, \n    \"end_profile_step\": 5, \n    \"tuner_type\": \"gridsearch\", \n    \"tuner_early_stopping\": 5, \n    \"tuner_num_trials\": 50, \n    \"model_info_path\": null, \n    \"mp_size\": 1, \n    \"max_train_batch_size\": null, \n    \"min_train_batch_size\": 1, \n    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n    \"min_train_micro_batch_size_per_gpu\": 1, \n    \"num_tuning_micro_batch_sizes\": 3\n}\n[2023-12-08 02:05:56,371] [INFO] [config.py:983:print]   bfloat16_enabled ............. False\n[2023-12-08 02:05:56,375] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False\n[2023-12-08 02:05:56,379] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True\n[2023-12-08 02:05:56,384] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False\n[2023-12-08 02:05:56,388] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7d93eec898d0>\n[2023-12-08 02:05:56,390] [INFO] [config.py:983:print]   communication_data_type ...... None\n[2023-12-08 02:05:56,394] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n[2023-12-08 02:05:56,397] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False\n[2023-12-08 02:05:56,400] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False\n[2023-12-08 02:05:56,405] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n[2023-12-08 02:05:56,408] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False\n[2023-12-08 02:05:56,412] [INFO] [config.py:983:print]   dataloader_drop_last ......... False\n[2023-12-08 02:05:56,416] [INFO] [config.py:983:print]   disable_allgather ............ False\n[2023-12-08 02:05:56,420] [INFO] [config.py:983:print]   dump_state ................... False\n[2023-12-08 02:05:56,422] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n[2023-12-08 02:05:56,427] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False\n[2023-12-08 02:05:56,431] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1\n[2023-12-08 02:05:56,435] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer\n[2023-12-08 02:05:56,439] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0\n[2023-12-08 02:05:56,442] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100\n[2023-12-08 02:05:56,446] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06\n[2023-12-08 02:05:56,450] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01\n[2023-12-08 02:05:56,454] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False\n[2023-12-08 02:05:56,458] [INFO] [config.py:983:print]   elasticity_enabled ........... False\n[2023-12-08 02:05:56,462] [INFO] [config.py:983:print]   flops_profiler_config ........ {\n    \"enabled\": false, \n    \"recompute_fwd_factor\": 0.0, \n    \"profile_step\": 1, \n    \"module_depth\": -1, \n    \"top_modules\": 1, \n    \"detailed\": true, \n    \"output_file\": null\n}\n[2023-12-08 02:05:56,464] [INFO] [config.py:983:print]   fp16_auto_cast ............... False\n[2023-12-08 02:05:56,466] [INFO] [config.py:983:print]   fp16_enabled ................. True\n[2023-12-08 02:05:56,471] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False\n[2023-12-08 02:05:56,474] [INFO] [config.py:983:print]   global_rank .................. 0\n[2023-12-08 02:05:56,478] [INFO] [config.py:983:print]   grad_accum_dtype ............. None\n[2023-12-08 02:05:56,480] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 1\n[2023-12-08 02:05:56,485] [INFO] [config.py:983:print]   gradient_clipping ............ 0.0\n[2023-12-08 02:05:56,489] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0\n[2023-12-08 02:05:56,493] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n[2023-12-08 02:05:56,494] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 65536\n[2023-12-08 02:05:56,497] [INFO] [config.py:983:print]   load_universal_checkpoint .... False\n[2023-12-08 02:05:56,499] [INFO] [config.py:983:print]   loss_scale ................... 0\n[2023-12-08 02:05:56,501] [INFO] [config.py:983:print]   memory_breakdown ............. False\n[2023-12-08 02:05:56,503] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False\n[2023-12-08 02:05:56,505] [INFO] [config.py:983:print]   mics_shard_size .............. -1\n[2023-12-08 02:05:56,507] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n[2023-12-08 02:05:56,509] [INFO] [config.py:983:print]   nebula_config ................ {\n    \"enabled\": false, \n    \"persistent_storage_path\": null, \n    \"persistent_time_interval\": 100, \n    \"num_of_version_in_retention\": 2, \n    \"enable_nebula_load\": true, \n    \"load_path\": null\n}\n[2023-12-08 02:05:56,511] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False\n[2023-12-08 02:05:56,513] [INFO] [config.py:983:print]   optimizer_name ............... None\n[2023-12-08 02:05:56,515] [INFO] [config.py:983:print]   optimizer_params ............. None\n[2023-12-08 02:05:56,517] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n[2023-12-08 02:05:56,519] [INFO] [config.py:983:print]   pld_enabled .................. False\n[2023-12-08 02:05:56,521] [INFO] [config.py:983:print]   pld_params ................... False\n[2023-12-08 02:05:56,523] [INFO] [config.py:983:print]   prescale_gradients ........... False\n[2023-12-08 02:05:56,526] [INFO] [config.py:983:print]   scheduler_name ............... None\n[2023-12-08 02:05:56,528] [INFO] [config.py:983:print]   scheduler_params ............. None\n[2023-12-08 02:05:56,530] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32\n[2023-12-08 02:05:56,532] [INFO] [config.py:983:print]   sparse_attention ............. None\n[2023-12-08 02:05:56,534] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False\n[2023-12-08 02:05:56,537] [INFO] [config.py:983:print]   steps_per_print .............. inf\n[2023-12-08 02:05:56,540] [INFO] [config.py:983:print]   train_batch_size ............. 8\n[2023-12-08 02:05:56,542] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  4\n[2023-12-08 02:05:56,545] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False\n[2023-12-08 02:05:56,549] [INFO] [config.py:983:print]   use_node_local_storage ....... False\n[2023-12-08 02:05:56,552] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False\n[2023-12-08 02:05:56,554] [INFO] [config.py:983:print]   weight_quantization_config ... None\n[2023-12-08 02:05:56,562] [INFO] [config.py:983:print]   world_size ................... 2\n[2023-12-08 02:05:56,565] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  True\n[2023-12-08 02:05:56,567] [INFO] [config.py:983:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n[2023-12-08 02:05:56,569] [INFO] [config.py:983:print]   zero_enabled ................. True\n[2023-12-08 02:05:56,572] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True\n[2023-12-08 02:05:56,576] [INFO] [config.py:983:print]   zero_optimization_stage ...... 2\n[2023-12-08 02:05:56,580] [INFO] [config.py:969:print_user_config]   json = {\n    \"train_micro_batch_size_per_gpu\": 4, \n    \"gradient_accumulation_steps\": 1, \n    \"fp16\": {\n        \"enabled\": true, \n        \"loss_scale\": 0, \n        \"loss_scale_window\": 1000, \n        \"min_loss_scale\": 1, \n        \"hysteresis\": 2\n    }, \n    \"zero_optimization\": {\n        \"stage\": 2, \n        \"offload_optimizer\": {\n            \"device\": \"cpu\", \n            \"pin_memory\": true\n        }, \n        \"allgather_partitions\": true, \n        \"allgather_bucket_size\": 2.000000e+08, \n        \"overlap_comm\": true, \n        \"reduce_scatter\": true, \n        \"reduce_bucket_size\": 2.000000e+08, \n        \"contiguous_gradients\": true\n    }, \n    \"activation_checkpointing\": {\n        \"partition_activations\": true, \n        \"cpu_checkpointing\": false, \n        \"contiguous_memory_optimization\": false, \n        \"synchronize_checkpoint_boundary\": false\n    }, \n    \"steps_per_print\": inf, \n    \"wall_clock_breakdown\": false, \n    \"bf16\": {\n        \"enabled\": false\n    }, \n    \"zero_allow_untested_optimizer\": true\n}\nEpoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/13 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[2023-12-08 02:05:58,088] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\nQ\n\n the   U.S. Supreme sector added aated growth in the first straight month. with that weakness continued weakness in the manufacturing’s manufacturing.\n\n2  new- of week:\n\n\nj.  employment inated  the  and first quarter quarter of growth employment and the was the second sign that a- in the manufacturingpts.\n.\n\nQ\n\n.1 is the data to\n\nA. What there any other signs?\n\n3. What is the fees of operation?\n\n4. What there any other tours??\n\n5. What I go a of\n\n6. Can there any stops??\n\n7. Is there a place certificate?\n\n8. Is there and be available? the\n\n1. Is many I get a the place location?showaction?\n\n10. How there a way in?\n\n11. Is I park a ticketroller?carchair/\n\n12. Can is the rules for parkinging with people?including you)?\n\n13. What there any rules animals? for\n\n14. What isits are in open??\n\n15. What is are being? the?\n\n16. What there any others? for childreniors?childrenents?\n\n17. What there any discount that eventsivals/ in?\n\n18. Do I buy a equipment?\n\n29. Will many will it take to get a?\n\n20. How there a way to achure??\n\n you ever aying the events? see information information? the asked questions? questions? may? they?\n\n a example,,? how' find a for preferences decisionsations.\n I I is important good good idea to have the experiences and their information. the asked questions. questions. may. they.\n is be a information to help you the experience experience.\n\n to I find a difference decision of questions most important questions?? a visitors? \n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/70017220.py:48: UserWarning: An error occurred while logging to Weights & Biases: You must call wandb.init() before wandb.log()\n  warnings.warn(f\"An error occurred while logging to Weights & Biases: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Step 0: Loss: 6.3475, BLEU: 9.1678, Perplexity: 571.0626\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/70017220.py:48: UserWarning: An error occurred while logging to Weights & Biases: You must call wandb.init() before wandb.log()\n  warnings.warn(f\"An error occurred while logging to Weights & Biases: {e}\")\n  8%|▊         | 1/13 [00:03<00:45,  3.82s/it]","output_type":"stream"},{"name":"stdout","text":"Step 0: Loss: 7.0674, BLEU: 15.2535, Perplexity: 1173.1326\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 1/13 [00:01<00:19,  1.59s/it]","output_type":"stream"},{"name":"stdout","text":"[2023-12-08 02:05:59,529] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\nQ\n\n iss a good code code: I the_-. run if- performance... on of the'.\n\n`\n\n2\n run_user_price_idping_self_\n    return check a for\n    for_id = _1_$ your amount of money transaction\")\") +\n    return_id = transaction$50 _price\n    return_id = = ._Payment not transaction have me transaction consent to the for transaction?\"\")i)y))\")\")       customer =>ation for the the the is the consent to\n    # transaction hasident is Y/     return_request)  customer amount of the transaction is the, $1_id} _id_110 +\n\n\n } total_idult = $othing\n        return_$ customer is accept the transaction, the permission_\")\n\n print false\n\n }\n\n\n    print(\"The customer intotxt enter again\");\n\n print false\n\n print    print Ifform the transaction test\n the the to they have to use\n the transaction.\n    end with .$ you want to proceed the transaction?\")i)y))\")\"    return (ing nil1/ && '\n 'y'\n    return. '.Y transaction\")txt enter a/ a/ no\") all\")\") x           \n        if ( == Y'\n        return Ifual transaction of be be place\n\n        if\n        else\n\n\n\n- Statement script toippet to getout\n Marketping\n\n\nor\n theers\n aning\n\n using followingumer form the the payment youwhen. if toset.. make the types.\nercode- if use a. the decision.making process. how to process is is handled.\n\nQ\n\n is the best for govern and for her use year? the new? and what? they need to be in?\n\n\n\nui What a new to the new questions?:  first is with a and,Jatalola Kidman) andaking up in a dream and She is a1995 and the City and the father is a a the. he is a to be home. and though he family is ended. He is with a small house beautifuly house with with her husband children. and andJisonaz)) and John (Jose Cley). She, she the year ago so,, she of children of disappeared and the left the. and any trace of a their belong. They of aretheice,iamia,Mis),idy), a. Diffle (Jll Dkes), and Mrs. Try (. (Mrednula Spenceravagan)the to the rescuestep and find to the emergency for wasney had sent on her customers. She are all in in the house, the, and is that situation of\n of be be a door to a the door door. and the othere are be be open. of husband are nottooobque\" andthe are the of theags, the on. they are not to the. than the light.\n of, she is to the houseman and says a the mail \"'s received to to her family werecing the she gone there. She's her. Hae, elderlyily and and that know why she are.and then they they get that her fact they they people was't been been out yet?\n. Mott is asks her that she the of them have been the work in the same. and that been the. much that they had back to the her they can be more service help to and they wayw a as the Rock's. had to attention.  they was them, that would a the her question, M were kept not. to be her..\nange, happen to the house, The they they is people who about theying, talking. sometimesies and tearsises. from the rooms. and and about her old man, and,Jis)ann) who theists on he is no in in with house. She isuses to let that husband and she asks her that stop. goish her. notbeing\" and. Marae saysures her that she is she husband her she she husband will\nStep 0: Loss: 7.7450, BLEU: 5.4543, Perplexity: 2310.0383\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 2/13 [00:05<00:26,  2.41s/it]","output_type":"stream"},{"name":"stdout","text":"Step 0: Loss: 3.6470, BLEU: 7.8733, Perplexity: 38.3579\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 2/13 [00:03<00:16,  1.50s/it]","output_type":"stream"},{"name":"stdout","text":"[2023-12-08 02:06:00,917] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\nQ\n\n:\n,name,\n name,,2 York,1\n1ed number to to to aSV:\n   \n \"  \"name\": : \"C\",\n\n  \"name\" : 2,,\n  \"name\" : state York\",\n \n\nQ\n\n is a list solutionF- theermination therem Sting. a\n`\nite\n TABLE IN\n\nISTS\n_1  IN FROMV_IMARY KEY,\n1    IN_name,_ NULL,\n\n    last_name NOT NOT NULL,\n\n    last NOTid_dateirth_AT, NULL, \n    last_ARACTid, NOT_1,TE1ale 'M', ) \n    age =F_ NULL, \n    age INAL ID NULL,\n   \n\n\n TABLE IN NOT NULLISTS (_\n    name FROMV_IMARY KEY,\n\n    name_name,, NULL,\n\n    last_name, NOT NULL, \n    last__ NOT  NULL, \n    last_ NOT NULL,\n   \n\n\n TABLE TABLE NOT NULLISTS\n_\n    IN FROMV_IMARY KEY,\n\n    id_name,, NULL,\n\n    last_name, NOT NULL, \n    last_NERGER_ NULL, \n    id_ NOT NULL,\n   \n\n\n TABLE IN NOT NULLISTS\n\n\n    IN FROMV_IMARY KEY,\n\n    id, NOT NULL,\n\n    name\n NOT NULL,\n   \n\n\n TABLE IN NOT NULLISTS\n\n\n    IN FROMV PRIMARY KEY,\n\n    id, NOT NULL \n\n    id\n NOT NULL,\n   \n\n\n TABLE IN NOT NULLISTS\n_\n    IN FROMV PRIMARY KEY,\n\n    id_ NOT NULL \n\n    medicalage, NOT NULL, \n    medical, NOT NULL, \n    serial NOTNERGER, NULL, \n    serial\n NOT NULL, \n    data_id,TEGER NOTQENCES\n_\n, \n    patient_id INTEGER NOTPORTENCES patient_id, \n    patient_id INTEGER REPORTENCES doctor_id,\n INVICE\nAREUALDE,   \n\n\n TABLE IN ( NULLISTS\n_id  IN FROMV_IMARY KEY,\n\n    id,,AMP_ ,ONE, NULL,\n\n    date NOTid,_ NULL, \n    time NOT NOT NULL, \n    time_id,NERGER,QENCES,_\n, \n   \nStep 0: Loss: 7.7366, BLEU: 9.1949, Perplexity: 2290.7251\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 3/13 [00:06<00:19,  1.94s/it]","output_type":"stream"},{"name":"stdout","text":"Step 0: Loss: 6.6385, BLEU: 7.3489, Perplexity: 763.9364\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 3/13 [00:04<00:14,  1.44s/it]","output_type":"stream"},{"name":"stdout","text":"[2023-12-08 02:06:02,290] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\nQ\n\n is a information code for the#her. get the fileo4j server. on the it oric is or. the after of the environment. possible effective.\n\n1<  the database.\n\t// TABLETAINTS\n\n_ .\n\n_\n\n_\n\t// INSTRAINT ON (node:Nodeationshipability) OR node IS NODE;\n\t\t\t\t The a with the node\n\t\tATCHGE_node))1ics)\n:Pended\"lingitness\" }\n\n\t\t =REATE TABLE\n\n\n\n\n \".\n2.id =at, \"_\n\t\t CARK_ topic1.id_at = NULL()\n\tON\t\tATCH =_\n_) \"ics)\n:Mfinite the\", }\n\n\tMLINEREATE TABLETING.\n\n\n \".\n2.id_at, .\n\t\t CARK_ topic2.id_at = ()\n\tON\t\tATCH = =1_) \"ics)\n:Mlook\",\", }\n\n\tMLINEREATE TABLETING.\n\n\n \".\n3.id_at, .\n\t\t CARK_ topic4.id_at = ()\n\tON\t\t  a between the\n\n\t\tATCH SET\n), \"ics)\nt1: Topic)\n\n2:1 =Ting\"\"\n t2.category=\"T\"\"\n\n\tondGESTt2:11ATEDIVE]NAMEOL(t1)\n\tT\t\t T aability to to the types\n\t\tERGE_tceiveable information) Tationshipability)\n: T10\n: \"Melcome a\" you a materials is description: \"I of the\") no aceries, }\n   \t =REDATE TABLETINGability\n.\n\n .\nability2.id_at, .\n\tRE CARK_ ability2.id_at = NULL()\n\tON\t\tATCH =_1ceable).1iability)\n: 10 data: \"M a tofree methodsitization to description: \"Iurance the\", \") home\", }\n \t =REDATE TABLETINGability\n00\n .\nability2.id_at, .\n\tREQ\n\n. {ative effectieldp\n\n\n1\nI,, I' to great bad experience with the point. I restaurant was terrible, theasty like. The food was terrible and the food was not happy. The was not surprised with I like have it restaurant to anyone. I\nI. Theitive reviewselp reviews\n\n1\nIed, I was to amazing experience! the restaurant. I food was great, the food was great. I food was great and friendlyentive. The was have recommend it place to anyone who for a place place. a great meal experience.��� a review50\nayp..I). 20\n review2.\n \n\n\nStep 0: Loss: 8.4526, BLEU: 7.5978, Perplexity: 4687.2593\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 4/13 [00:05<00:12,  1.41s/it]","output_type":"stream"},{"name":"stdout","text":"Step 0: Loss: 5.9222, BLEU: 8.6613, Perplexity: 373.2202\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 4/13 [00:08<00:15,  1.72s/it]","output_type":"stream"},{"name":"stdout","text":"[2023-12-08 02:06:03,689] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\nQ\n\n is a list solutionF- of the a system..\n\n`\n.\n TABLE\n\n NULLISTS\nSELECT_t'ons`'s`\n\n  `id` = `id)0 )PDIDED)O_INCREMENT,IMARY KEY,\n `  `id_ INSAMP__ORRENT_INSAMPABLE,PDATE,URRENT_TIMESTAMP ON \n  `idlesring_ INOST__ \n  `wistent` CMP_C') 'NO') \n  `REVER(', 'Eent`\nQENCES\nisent`\n0,\n\n \n\n`\n\n the is the code code code: I the query query of: on the query::\n\n`\n.\n *\n\n FROMUNT(SELECT\n _id,     FROMUM(*)(1ensus_ SELECT\") \" =ring_ \"1, AS ,count,\n    SUM(number(argent = nullNo\",))DER)), washed, 0)) as None_count,    (ts\n._.ss\n\n\n`\n\n\n，ed new to with store query toippet\n theendar theended\nastts\ndriene.\nlingashing\n- the Tableilet\n the. thening.\n thisporate the youthen into the toset to into the the types of to the dataumer of\nadcheck- and that application is is is correct. correct-defineded.\n\nQ\n\n be the effect method- the of to the used to know the factors factors of on the age. The\nThe. Introduction the the10 wereigate their 10, then farmer have 1000.0 the will be no more of to the losspletion.\n cost will the farmer will $1000.\n\n2. The the10 areigate  10, 10 forrigates, 1 months, the farmer will hasrigates will 1 months will be 1000.0 the farmer 20 will receive $1000..\n cost of the next will isrigates the 10 will 1000.\n cost for the farmer farmer: $1000..\n\n2. The the10 areigate  10, 1 months forigate for 1 months, the farmer will hasrigates  1 months will receive 1000.0 the farmer farmer20 will receive $1000..\n cost of the farmers will areigate the 10 will $1000..\n cost for the farmers : $1000 each.\n\n2. The the10 areigate  10, 20 forate for 1 months, the farmer will burigates  1 months will receive 1000.0 the farmer farmer30 will receive $1000..\n cost of the farmers will areigate the 1 days will $1000..\n cost for the farmers : $1000 each.\n\n2. The the10 isrigates  10, 10 forigate  1 months, the farmer will hasrigates for 1 months will receive 100..0 the farmer farmer will receive $1000.\n cost of the farmer will produceigate the 10 will $1000..\n cost: the farmers : $1000.\n\n2. The the the10 wereigate their 10, the farmer will be pleted. and the will be be from loss of 1000..\n cost will the farmer will $1000.\n\n3 the beginning mentioned, the is possible that the farmers amount to farmers farmer is to pay a least \nStep 0: Loss: 8.9541, BLEU: 7.3096, Perplexity: 7739.6084\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 5/13 [00:09<00:12,  1.60s/it]","output_type":"stream"},{"name":"stdout","text":"Step 0: Loss: 6.0892, BLEU: 7.1790, Perplexity: 441.0794\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 5/13 [00:07<00:11,  1.41s/it]","output_type":"stream"},{"name":"stdout","text":"Q\n\n iss a good code code: I the making in on the or are aming or a.\n\n//\n.2\n test iftheestlight_(self,om,\n    if\nermine the- density for usinging the of       def\n\n        for:1)1] =\n[ symptoms\n a performance care\n               def:\n        return =\nicate: of nail damage\n\n1med 'test', => 'dral\n\n\n\n returnisk:\n        str:\n the is a or,       \n        return\n    ifinclude if for for\n    for input required typestring,om) int)\n\n\n        #_(1\")\")\")\n\n           # Check a for parameters values\n\n    # = \n        [_\n, s\")\", ] \", range.\n\n => =\n\n(    # =s == \"Yush\" for s in sympt]\n #\nittle\n ->\n\n enced        #\n    #: [0',color',test',\n       #\n theitional\n to set a\n\n    # ( are0]\n        return: '.0]:\n        else result[0]:\n        return = actions[1]\n        }:\n        result = actions[1]\n           return result\n\n      `\n\n\ned new script\nippet\n getermine the\nodesupt\niggerble theail\n theimal \n aessionals.\n thisporated the thewhen. the toset.. the the types. to the applicationationshipability of\nadcode: but that code is is is correct. correct-defineded.\n\nStep 0: Loss: 5.7507, BLEU: 9.1894, Perplexity: 314.4258\nQ\n\n1)\n. 1\nThe the- solarosion are common common that are in timeologicalic time, they are not always only ones that occur in ground ground of the earth. Thescolution is for is the in the shape structure of the species, time, is plays in timeologicalic time. the level of the species. The, the number detailed description is the question of be toHowuchscolution\". the factorsolog processes\". as ge and waterosion\".\n\n we think that answer way would this question wouldM is when timeologicalic time\" the ground of the Earth?\"  correctthes waterosion\"? or that the1bolution isapolates from in geologic time? the level of the species?\n\nA2)\n,\n\nB) Yes.\n1","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 6/13 [00:09<00:11,  1.59s/it]","output_type":"stream"},{"name":"stdout","text":"\nStep 0: Loss: 6.4105, BLEU: 11.2904, Perplexity: 608.1783\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 6/13 [00:11<00:12,  1.73s/it]","output_type":"stream"},{"name":"stdout","text":"Q\n\n a example,,, it have find the test a of that\n, I of way is to use a sametest''` functionnotation to\n can use a class class,, using a new one to to the objectnotation. but this:\n\n//`\n\ninterface((Errorried:old:\n\n`\n\nThe, the the code,, you can use the following to use theErroremporaryHack\" as a variable.\n\n```\n\nscriptap>>\n\n <h>>=\"1_\"1emporaryHack\"\n    <</\npackageac>\n\n`\n\nThe is be the following to run the warning message the errors that errorated with the````.ITErrorours\").`\n\n\n->\n'm using to write a error error exception. my to get with this bug problem. I' for the update debug to be loaded. I I a from theing the as a ' ' string, a warning of others of I'm like to see a code do a exception exception.. well warning to that can't have to check a as of\n now, if like:script>\n\nAp>a>\n0ap]</>google.baskage.h</12901: '::ing prevent on this error. but thega I is not.\n\nscript>p>\n\nAcode>\n there a way to can fix this error to error to to the warning like \" own?\n\nailed to the I cans the best way to do to the list? make at exception error? and the a few like the file? the wrongending file? that doesn deleted out the same??\nscript>\n\nAp>\n: IIending::'t work to work working the wrong me.p>\n\nAp>p>\n\n */ @author */ErrorIT\"\" make with the code errorotaks.\n */\n\nimportrecated(@ void on_(at((uff( {\n }\n{script>code>\n\n code>\n, error compiler.. thisclipse. other the.avascript.\n.1.2 on iv) but no iss not notable on same.p>\n\n\nQ\n\n\n a following\n\n_atres_ 1\n\n0\n =co = = 1\n\n\n_elpl = 14\n\n__minork = 1\n1\n\n#  the number of coins in areount uses\n\nins =fort = _ofins_ \nofire\n 1_am)) _amique)\n\n#  the number of coins inount usesends on\nins_amend = _amaze_ _amork *\n# We the number of coins_alie * in:#ins_amaining_ coins_amime * _deent\n\n# Cal the number\n\n_1)1ins_spain)\n\n\n0 number of coins to arese has to  number isount has to 0/010:\n she number number of coins is have to 1,,, then thealie hasends 103 of the she has, her, then many coins she have in?\n help to email a comment to El.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 0: Loss: 3.7884, BLEU: 4.4925, Perplexity: 44.1841\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 7/13 [00:11<00:11,  1.94s/it]","output_type":"stream"},{"name":"stdout","text":"Step 0: Loss: 3.5066, BLEU: 5.4362, Perplexity: 33.3349\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 7/13 [00:14<00:12,  2.03s/it]","output_type":"stream"},{"name":"stdout","text":"Q\n\n is like a is be a new between the new of.. the file version.nameations_.\n\nI of is be to use the names_migrations table in replacereate the with the new name table.e.g. anameils`` table migration.igrations.`).\n is be the the errorsting information patterns. make the to create them__apiations` in.\n\nTheother way is be to use create the migration entries from the migration.pathate_.\n\nA you the this migration is is is is not used to themigrRecord`Base`Base``jsonations`path` it is be that to the fact the migration is is is being up.\n this the information the documentation, the files I iss possible to see what sure.\n\nhttps>\n is the code:json\n a look onlineails project...  Ifp>\n\n p>\n' the the lotR\"\"\" file of the,modules.  It is no \"database\"dbate\" option in the or file. a \"db/migr\"xml\" folder.  The migration is is is a a singlemed down versionails application. it have theails  we wanted toails to. be us do createate top>\n\n p>\n R of The file is looksreates the R R schema.with' to use it old schema clean the separate file, to the a tables). are to be the database). re database to use the database). with well).\ntable>\n\n p>\n you have thisa>\n</</1_code> then want the that thecode>my</namement_code> is Icode>app_develop</code> will.\n\n getm got created the database andidate_ created_test. from the the  my,_id_ I there. I the data.p>\n\nAp>\n you have myails__configate from I get the thatp>\n\n<p>p>\n</</Baseplicate_</</</ <1<< records: been same ofAppNameName.ank()ahBl\npre>code>\n\nIdiv>\n can it possible? how can I fix around it?? I can fixate my\np>\n\nAp>Can the point, tried not this migration pull, would the database\nQ\n\n is a list solution of the function. the.\npublic`\n:\n java.util.\n\n class TestermTestR { {\n    public static void main(String[] args) { Exception {\n        this  the from the\n        //anner. = new Scanner(String.out.\n        //.out.println(Hello a time\");0) T0\n        System resultTime = TimegetTime();\n        System.out.println(\"Start start time:MD:MM:MM\n        System startTime = \".startLine();\n               System  the to numbersgers\n        // iTime = = .length(1Time);length(1, 00\n        return startTimeute = .toString(startTime.length(00 ,\n        return startTimeours = new.length(startTime.length(start,010\n        int startHute = True.toString(0Time.length(00 0\n       \n        if culate the between the\n time\n        return countTimeute = 1Timeours. Timeour)100 endHutes) startHutes)\n               for  the the is a time to the\n        // (startTimeute) 1))\n            //.out.println(Time,\n        }\n if\n            if.out.println(\"Yes\");\n        }\n           }\n\n\n\n`\n\n  new application ippet\n willermines the-ation\n\n staticit\nensorules\n theers\n aimental \n thisporate the necessarywhen into a toset to. the the types. on the type of\n that application is is correct defineddefineded. the. the own.\n\nStep 0: Loss: 2.5612, BLEU: 5.8717, Perplexity: 12.9510\nStep 0: Loss: 1.8921, BLEU: 12.3628, Perplexity: 6.6332","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 8/13 [00:13<00:09,  1.95s/it]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 8/13 [00:15<00:10,  2.01s/it]","output_type":"stream"},{"name":"stdout","text":"Q\n\n.J. death died killedified home,��� help the following questions:\n am have the I. I father was that I she was ad. had going longer able child. She was a first of   I mother were still sure home. I was the difficult and I was not about going some else I help my spirits temperature. ., I I am I have do done to it. the.\n only is very for andaking...fi, was , I was was very. I was me a an hour to find the newographic. I a the movie, wased it and Iords? am you friendsbud ? Iried me10 minutes to get the phonephones. I video was with I started the1pm I screenbell rang. I was the I found theed. I got to open the noise. I wife were very in and I was in home my room. I wanted to the movie. I I have to new of I I am watching, am getting movie in my car room. I phone is very the bathroom room as I movie was in I I the phone part. I mom is a text Iapp message. I phone was it it. on. I any her indicationningings the phone. said the phone and called was gone phone.. I was no way.. I phone was starts playing I a1 minutes it was the phone out the room. said sheI is my you said doing.” he iss not”. and mom are and and they wrong?\n was “ wass a movie game and my phone. they was with. I was a to say but the.. I I phone is, to did going. I iss a video. the company. I me to stop it video. I dad said me a I was had what I was it video of video. I the moment on, I have thoughted video. I never them on. and never watch watching moviesering..\n I you movie of the movie?le Movie movie to the internet link?\n1.I I son were in the movie,1. When he dad were in 1. When he dad had asleep in the  C. when when to to\n:\n\n\nStep 0: Loss: 2.5911, BLEU: 7.4224, Perplexity: 13.3449\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 9/13 [00:16<00:08,  2.21s/it]","output_type":"stream"},{"name":"stdout","text":"Q\n\n a was through the door, I first of already and. and a perfectical glow. I me field and to if they were in. the air. The��, the of of\n of a, wind,\nThe is the tree?? trees?\n\n\nStep 0: Loss: 1.6579, BLEU: 5.6515, Perplexity: 5.2485\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 9/13 [00:18<00:09,  2.26s/it]","output_type":"stream"},{"name":"stdout","text":"Q\n\n is a listizarre script that willates the-ity insuranceupss. to theworth. the the checks to on the patient. the.\n script is the$` to to calculate if the patient risk is below the parameters.ily low, low)\n the, the the will the  that that value value of times checkspointsins. with the value.\n, the wills the user that the one check is.\n\nThe`\nash\n\ninclude/bin/bash\n#!s:\nendar theH_Health_ps_\n\n#\n:\n script willates the number number of a-. on the's age history. the information..\n## The more from the.\n \"1 yourient Name Information\"\n\n inputial\n\n \"Pat Pat Statisticsage\"\"1] anned] 2 - Pl] 3 - Silver]\n\n\n ins letter\nechoThe  the\n\nianor11))\n\nuggest the\n $\n \n\n $oresss)00000 100 \n S $ the array\n array\n\n the =\n\n\n of check the\n the\n\ncheck# op to the the\n the records\n\n\n each in range$__li\n\n\n    data = the  for$lightanceterol\"\n  \"_\ni]11\n    check\n\n    returnCagnetic\"oleonus\"\n    \"   [0]=1\n      check;\n      }Dipertension\"\n    check   [2]=1\n      check;\n      }\n    check \"1\" found\"\n    echo\n\n\n    echo;\n     lint_   \n  includeermine the score of\n on thepoint.    = score=1\n\n i-= 0; i < i is.ii +)++));\n\n   _=1i_[i]};i[ =[i]\n \n\n  Test the the is is below the\nresholds\n\n (%i = score]1ak1]];];]\n\n   [ =0\" test\"\n} fdisom =$ a newized test of per month\"; a of the training\" care\"\n  testTestr =]]1 1]]] $r\nStep 0: Loss: 1.9528, BLEU: 5.0845, Perplexity: 7.0484\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 10/13 [00:20<00:06,  2.22s/it]","output_type":"stream"},{"name":"stdout","text":"Q\n\n $ =  positive of  of10 Let have that the are  groups ways of  4. to  .\n groupic and and and and the cycl group.way C.. We\nThe, we uss say the followingic group C4.\n members are be divided by a1, , b,2, a,2, a the is a, of the group C\n groupplication of is the is is a follows:\n\nC`\n\n\n  | | | |1 2  a^2   \n\n+\n |    |  a^ a^3  a^3\n ^  1^^3  a^3\n a  ^3 \n^3\n^3\n1\n\n^a^2\n a^3\n a\n1^\n^3\naa\n\nA, I's say the following--row of,.\n a are be divided by:1, , b, b, and the is2 is ,2 + .2. b1. aj ^ a = ^ b. ^ c = ^ c =  + b = b,\n firstplication of is the is is a follows:\n1A`\n\n1  | | | |1  |   \n1+\n |   |  b  b  c\n     b 1  b\n c\n \n  b\n c\n1  b  b    b\n c\n c\n1  b`\n\nA get the the is a a to the,, not,, I have to define the number of the and\n we is a integral of  1, we we is aomorphic to G4.\n G of-integer elements of order are a 4, then G is aomorphic to G..\n\nA $s say the following of G and follows\n, , , z,\n the is a of1, we can that G G of of4 is a the'\n we we can to know the identity  G and y, and z in\n\nA. What we is a element   of1, then we, aomorphic to x..\n G of orderality, G xs say that is a 4.\n we we is x, , ,2, x,2, then\nStep 0: Loss: 2.6663, BLEU: 14.3684, Perplexity: 14.3864\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 10/13 [00:18<00:06,  2.24s/it]","output_type":"stream"},{"name":"stdout","text":"Q\n\n: the problem, we have to find a right denom of the and and Set1. then them in\n is the elements: do this:\n\n1. Find a the elements set of\n\n\n1 = Set\n, 1, 5, 1, 10,\n   Set2: {3,, 1,\n    : Findify the number denom of the elements sets.\n the case, the first one elements is the1.\n\n3. Find the the elements elements of the list set. and will  first of the and and Set1.\n\n\nmediate \n\n,\n   . Find the elements of elements in the set of1\n the case, the are a one element in\n55 is, the number is this question is:\n is no1 element in the intersection set the and and Set1.\n\n\n\nermine Descriptionructions\n\n this case, the can given a options of one the have to add the number of elements in each end of each elements elements.\n\n set is a in the setsved brack. abs separatedseparated by are the respectively this1, 2, 3,\n set of these br br of a same. of is the the elements of are in in the sets.\n make the same of two sets sets, you and B, used function of of  the elements that to each sets and B. The\nblem  The is is A1:11, 5, 6, 10,\n1:Set3,, 1,\n\n do of are there in the set of the2 and 1?\n\nolve:\n\n\nQ\n\n, I can use this -> to</ to; to<gt;%& to a <code>C&code>\n using the followingcode>List-code>\n.\n\n\n  example, if's say I have a list that \"code> whiching. thatcode> that returns a listcode>data.</;\n<lt;d& and the that a database database database.\n can use this followingcode>Data</code> to to get for the data of happen the then return it to a newcode> query<code> element the it to the nextcode>List.idscode>\n\n code>List    <:code>\n_</ new((\n  new data data of  My<Data = = new MyData(); // a for the return be\n return to new;  List forsnewList MyDataList)\n\ntable>\n\n< is return that the have a listdata> <code> and the in be to the databasecode>List.listscode>\n.\n\n\n\ncode>\n want using the custom to thisipping. \"p>fl_widget</code> \n following is is the plugin is the number of\n the I am thecode>search.,code> I theing the, I get to set mycode>data.</;\n<lt;%>\n.\n\n there any way to can get thecode>Name&gt;&&gt;code& form acode>Name&code>\n\n, way way to do the data?code>\n\n\nStep 0: Loss: 1.3549, BLEU: 10.7122, Perplexity: 3.8764\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▍ | 11/13 [00:21<00:04,  2.26s/it]","output_type":"stream"},{"name":"stdout","text":"Step 0: Loss: 3.5318, BLEU: 6.8395, Perplexity: 34.1844\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▍ | 11/13 [00:23<00:04,  2.29s/it]","output_type":"stream"},{"name":"stdout","text":"Q\n\n19\n inte\n22 following of by the paper is that. it word isions that the word is in on theraal the character of the.. the film.The Last of All Fools.\"\n is that character is not make the fact's port in the film.\n author character is \"Theitive\" is to. the is no \" for criticism for the film.\n\n\nThe actor is is to the man to thetheerry theusementlo is who the theoy'' is into. the that.ries..etsides. the movie's new movie movie. '' the movie of the the and .\n\n\n do you movie of the movie be interpreted? and\n\": given\n\n1) \"\n\n (2)\n.\n\nx:\n\n\nQ\n\n is a list of of the a function: in Cavascript.\n\nfunctionfunction`\n\n\n\n \n\n\n// test((ieriene(int,om) {\n  //  the for\n  // $Hygiene = ;\n  //  // Create ifomatic for\n  // (hptom.hauss .haf)reatat) symptoms.s()at) symptoms.siddle ||)\n  else symptoms.cjion. symptoms.s\n ||\n\n    //  the of is not, the thegrom_\n thetrue'\n\n      //.ouiene status true;\n      //\n if\n      if Doim\n if options settings of\n      // { =ptoms = true.get.1ptoms.val(error) => {val)\n      //      if (isSymptoms) {\n        return Do the is no any errors\n\n the the the entries\n\n        // If the the again a. not not needclosefect\n\n\n        //_enceiene. false\n\n        //\n    //\n\n\n    trueHygiene.\n}\n\n  \n\n\n\n// { = = {\n: {, user:::: true,\nconst.log(functioncul(esttiene.1Input.\n\n\n true\nconsole  :age\n\n\n\n// { = = = {\n:umbx: true, errorTh: true,\nconst.log(thiscul(esttiene.1otherUserInput,\n : true,console  :age:\n:\n { = = = {\ncurrention: , con:: true,\n\n.log(\ncul(estigiene.1UserInput,\n \n false\n\n`\n\n// this case, have need a new that aUserHygiene` which is be the code..\n then use the arrayH */ */` to to calculate if the of of the values values isihauses `cighnessumbat` `ss`ight` `runuzz`` `fjion`)` `fache`` a in\n the, it can thef`auseiene` to thef`\n\nThe, the we of the conditions are present, we can to use if the than\nStep 0: Loss: 2.0358, BLEU: 7.4708, Perplexity: 7.6586\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 12/13 [00:25<00:02,  2.20s/it]","output_type":"stream"},{"name":"stdout","text":"Step 0: Loss: 2.8052, BLEU: 9.0214, Perplexity: 16.5300\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 12/13 [00:23<00:02,  2.19s/it]","output_type":"stream"},{"name":"stdout","text":"Q\n101 is the data to\n\nA. What there any other signs?\n\n3. What is the fees of operation?\n\n4. What there any other tours??\n\n5. What I have a of\n\n6. Can there any stops??\n\n7. Is there a place certificate?\n\n8. Is there and be available? the\n\n1. Is many I know a the place location?showaction?\n\n10. How there a way in?\n\n11. Is I park a ticketroller?carchair/\n\n12. Can is the rules for parkinging with people?including you)?\n\n13. What there any rules animals? for\n\n14. What isits are in open??\n\n15. What is are being? the?\n\n16. What there any others? for childreniors?childrenents?\n\n17. What there any discount that eventsivals/ in?\n\n18. Do I buy a equipment?\n\n29. Will many will it take to get a?\n\n20. How there a way to achure??\n\n you ever aying the events? see information information? the asked questions? questions? may? they?\n\n a example,,? how' find a for preferences decisionsations.\n I I is important good good idea to have the experiences to their information. the asked questions. questions. may. they.\n is be a information to help you the experience experience.\n\n to I find a list decision of questions most important questions?? a visitors? \nQ\n101 the code income quality index for\n water water have not to law to report water with a accurate water quality report.\n municipal are information about the water of water water quality the quality, and the quality of\n can also find information reports in the localality’s website. on callinging the office resource.\n\nThe. Field  local water authority for\n  local water authority to ask them the water of water water water.\n will tell able to provide you about water water quality issues. problems you your area.\n\n3. If the public meter kit to\n can use a water quality kit and test your water of your water water.\n testsits are designed in and in the stores.\n waterits are be your the levelsaminants and as leadlorine, lead, lead mer.\n\n3. Field the waterPA websites website Waterinking Water Qualityline for\n E Protection Agency ( issued list Drinking Water Hotline. is can use if check your about the water of your water water..\n Safeline is help used at 1-800-272-8277.\n\n3. Theang a Clean to You can hire a certified to work your water quality\n for a certified certified techn a certified-ofified certified water specialist specialist.\n can be able to provide you with a information about the water of water water.\n\n is can you take to I have a that you water water company cont safe? drink?\n\n you have out that your water water is not safe to drink, you is other ways you can take to\n\n1. Ident the app water of water. water you water is notaminated, you can use using water. you is be tested. used.\n aled water to water that a tap source. such as a tap- is a used for tested.\n\n2. Use the water to\n your local water authority or ask the problem to\n will be contacted to provide you with information about the problem. how possible you can taking to ensure it.\n\n3. Ifigate the water of\n to find if cause of the problemamination. your home system If is help you determine the water. theediation. treatment the problem. occurring..\n\n4. If the water in Ifending on the situation, the wateram, the source of wateraminationant, you may be a options..\n may use with the water to a a a localPA. determine\nStep 0: Loss: 1.6713, BLEU: 15.4710, Perplexity: 5.3191\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:27<00:00,  2.10s/it]\n","output_type":"stream"},{"name":"stdout","text":"Step 0: Loss: 2.0316, BLEU: 10.7990, Perplexity: 7.6262\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:25<00:00,  1.93s/it]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74d6d06b14784c338e8f2e2629859076"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fcb3ba4e363446c879d4488da1d7e26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd44d0d12b342beaf9f5930329be074"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcaeac2d1dcc40af9f06eafe7e8b85f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3188201be2b49e2a7ca49a6d7ac75b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3229d07e5d234d08b2807f7e4272eb67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cee1deb68c842c58561bfb14d593e76"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|██████████| 192/192 [00:23<00:00,  8.23it/s]\nEvaluating: 100%|██████████| 192/192 [00:25<00:00,  7.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: (4.566477949802692, 1076.327698304103)\nValidation Metrics: {'val_loss': 0.6206789101318767, 'val_perplexity': 1.860190510749817, 'val_bleu': 0.06140450433316078, 'val_rouge': {'rouge1': 0.3516021640861382, 'rouge2': 0.08372148884102157, 'rougeL': 0.28219738666653515, 'rougeLsum': 0.2859703612532966}}\nTraining Loss: (4.264929395455581, 579.314645602153)\nValidation Metrics: {'val_loss': 0.6355592787731439, 'val_perplexity': 1.888077735900879, 'val_bleu': 0.06304295801176084, 'val_rouge': {'rouge1': 0.3462716477013651, 'rouge2': 0.0813329311407697, 'rougeL': 0.2761206054090262, 'rougeLsum': 0.2813962596394301}}\nTraining complete!\nTraining complete!\n","output_type":"stream"}]}]}