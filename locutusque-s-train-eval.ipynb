{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PyTorch training script\nThe following notebook will demonstrate how I trained TinyMistral.","metadata":{}},{"cell_type":"markdown","source":"# Installing libraries\nWe'll install any dependency that we need for this notebook.","metadata":{}},{"cell_type":"code","source":"# I'll also be installing one of my other packages for a model that I have private.\n# This package contains nice utilities that I'd rather not code again.\n!pip install --upgrade sentia datasets transformers evaluate rouge_score\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom evaluate import load as load_metric\nimport sacrebleu\nfrom tqdm import tqdm\nimport math\nfrom sentia import SENTIAForCausalLM,\nimport wandb\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom typing import Optional\nfrom dataclasses import dataclass, field\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2023-11-20T23:58:21.045590Z","iopub.execute_input":"2023-11-20T23:58:21.046435Z","iopub.status.idle":"2023-11-20T23:59:09.978578Z","shell.execute_reply.started":"2023-11-20T23:58:21.046390Z","shell.execute_reply":"2023-11-20T23:59:09.977545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# String to dtype mapping\n\nWe'll define a constant to provide an easy way to define dtypes on the command line. (This would be a bit more relevant if it was not a notebook)","metadata":{}},{"cell_type":"code","source":"STRING_TO_DTYPE_MAPPING = {\n    \"bfloat16\": torch.bfloat16,\n     \"float32\": torch.float32,\n     \"float16\": torch.float16,\n     \"float64\": torch.float64,\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T23:59:09.980541Z","iopub.execute_input":"2023-11-20T23:59:09.980956Z","iopub.status.idle":"2023-11-20T23:59:09.987363Z","shell.execute_reply.started":"2023-11-20T23:59:09.980922Z","shell.execute_reply":"2023-11-20T23:59:09.985797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset classes\n\nWe'll define the dataset classes that will be used for data preprocessing","metadata":{}},{"cell_type":"code","source":"class ConversationDataset(Dataset):\n    def __init__(self, tokenizer, max_length=512, data=None, device=\"cuda\"):\n        self.data = data\n        self.device = device\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        try:\n            # Most of the time I'll be using InstructMix for instruction-tuning\n            user = self.data[idx][\"Input\"]\n            assistant = self.data[idx][\"Output\"]\n        except KeyError:\n            # If I'm using MMLU for evaluation\n            user = self.data[idx][\"question\"]\n            ans_index = self.data[idx][\"answer\"]\n            assistant = self.data[idx][\"choices\"][ans_index]\n        \n        input_text = f\"<|USER|> {user} <|ASSISTANT|> {assistant} <|endoftext|>\"\n        target_text = f\"<|USER|> {user} <|ASSISTANT|> {assistant} <|endoftext|>\"\n        input_ids = self.tokenizer.encode(input_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        target_ids = self.tokenizer.encode(target_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n        target_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(target_ids))\n\n        return {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.int64, device=self.device),\n            \"labels\": torch.tensor(target_ids, dtype=torch.int64, device=self.device),\n        }\nclass CompletionDataset(Dataset):\n    def __init__(self, tokenizer, data, max_length=256, device=\"cuda\"):\n        self.data = data\n        self.device = device\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data[idx][\"text\"]\n        input_text = f\"{text} {self.tokenizer.eos_token}\"\n        target_text = f\"{text} {self.tokenizer.eos_token}\"\n        input_ids = self.tokenizer.encode(input_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        target_ids = self.tokenizer.encode(target_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n        target_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(target_ids))\n        return {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.int64, device=self.device),\n            \"labels\": torch.tensor(target_ids, dtype=torch.int64, device=self.device),\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training loop\n\nWe'll define the training loop, with a few metrics to keep track of the model's learning.","metadata":{}},{"cell_type":"code","source":"def train(model, dataloader, optimizer, tokenizer, device=\"cuda\"):\n    model.train()\n    model.to(device=device)\n    total_loss = 0\n    total_perplexity = 0\n\n    for i, batch in tqdm(enumerate(dataloader)):\n                input_ids = batch[\"input_ids\"].to(device)\n                target_ids = batch[\"labels\"].to(device)\n                target_text = batch[\"target_text\"]\n                # Generate the output and calculate the loss\n                outputs = model(input_ids=input_ids, labels=target_ids)\n                loss, logits = outputs[:2]\n                # Calculate the BLEU score\n                probs = F.softmax(logits, dim=-1)\n                predictions = torch.argmax(probs, dim=-1)\n                predictions_str = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions.tolist()]\n                target_ids_str = [tokenizer.decode(tgt, skip_special_tokens=True) for tgt in target_ids.tolist()]\n                print(predictions_str[0])\n                bleu_scores = []\n                accuracy_scores = []\n                for pred_str, target_str in zip(predictions_str, target_ids_str):\n                    bleu = sacrebleu.sentence_bleu(pred_str, [target_str])\n                    bleu_scores.append(bleu.score)\n                for pred_id, target_id in zip(predictions, target_ids):\n                    accuracy = SENTIAForCausalLM.calculate_accuracy(pred_id, target_id)\n                    accuracy_scores.append(accuracy)\n\n                accuracy = sum(accuracy_scores) / len(accuracy_scores)\n                bleu = sum(bleu_scores) / len(bleu_scores)\n                # Calculate the reward\n                # This reward can be used for RLHF, but I prefer using it as a metric.\n                # The highest value is typically around 2x the sequence length.\n                # The lowest value is typically about the negative of the sequence length.\n                reward, penalty = SENTIAForCausalLM.get_reward(predictions.tolist()[0], target_ids.tolist()[0], bleu)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                # Update the metrics\n                total_loss += loss.item()\n                try:\n                    wandb.log({\"loss\": ol.item(), \"bleu\": bleu, \"perplexity\": torch.exp(ol).item(), \"accuracy\": accuracy})\n                except:\n                    pass\n                print(\n                    f\"Batch {i + 1}/{len(dataloader)}: Loss - {loss.item():.4f}, NetReward - {reward - penalty:.4f}, BLEU - {bleu:.4f}, Perplexity - {torch.exp(loss).item()}, Accuracy - {accuracy}\")\n\n    return total_loss / len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T23:59:09.988955Z","iopub.execute_input":"2023-11-20T23:59:09.990587Z","iopub.status.idle":"2023-11-20T23:59:10.016968Z","shell.execute_reply.started":"2023-11-20T23:59:09.990506Z","shell.execute_reply":"2023-11-20T23:59:10.015902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation loop\n\nWe'll define an evaluation loop with multiple metrics to track the model's performance. We'll include scores like loss, perplexity, bleu, rouge, and f1.","metadata":{}},{"cell_type":"code","source":"def evaluate(model, val_loader, tokenizer, use_cuda=True):\n    model.eval()\n    device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Load metrics\n    bleu_metric = load_metric('bleu')\n    rouge_metric = load_metric('rouge')\n    \n    # Initialize variables to accumulate scores\n    total_loss = 0\n    all_predictions = []\n    all_references = []\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n            # Move batch to the correct device\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            \n            # Forward pass\n            batch.pop(\"target_text\")\n            outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"])\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            # Convert logits to predictions (for F1, BLEU, ROUGE)\n            # This part depends on your model's output format and the task\n            # Here is a mock-up of how you might extract predictions\n            # For token classification tasks:\n            # predictions = outputs.logits.argmax(dim=-1)\n            # For seq2seq tasks:\n            predictions = tokenizer.batch_decode(outputs.logits.argmax(dim=-1), skip_special_tokens=True)\n\n            # Post-process batch to extract labels and predictions in a suitable format\n            references = batch['labels'] \n            references = tokenizer.batch_decode(references, skip_special_tokens=True)\n            \n            # Update metrics\n            references = [[ref] for ref in references]\n            bleu_metric.add_batch(predictions=predictions, references=references)\n            rouge_metric.add_batch(predictions=predictions, references=references)\n            # Store predictions and references for later use if needed\n            all_predictions.extend(predictions)\n            all_references.extend(references)\n    # Compute the metrics\n    bleu_score = bleu_metric.compute(predictions=all_predictions, references=all_references)\n    rouge_score = rouge_metric.compute(predictions=all_predictions, references=all_references)\n\n    # Perplexity can be calculated from the total loss\n    # For perplexity, we assume the loss is the negative log likelihood\n    # In case the loss function is something else, this needs to be adjusted\n    perplexity = torch.exp(torch.tensor(total_loss / len(val_loader)))\n\n    metrics = {\n        'val_loss': total_loss / len(val_loader),\n        'val_perplexity': perplexity.item(),\n        'val_bleu': bleu_score['bleu'],\n        'val_rouge': rouge_score,\n    }\n    try:\n        wandb.log(**metrics)\n    except:\n        pass\n\n    return metrics","metadata":{"execution":{"iopub.status.busy":"2023-11-21T00:07:13.530567Z","iopub.execute_input":"2023-11-21T00:07:13.531538Z","iopub.status.idle":"2023-11-21T00:07:13.543883Z","shell.execute_reply.started":"2023-11-21T00:07:13.531500Z","shell.execute_reply":"2023-11-21T00:07:13.542883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TrainArgs\n\nHere we'll define a class creates a config for the training.","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass TrainArgs:\n    # Model configuration\n    model: str = field(default=\"Locutusque/TinyMistral-248M\")  # Pretrained model name or path\n    batch_size: int = field(default=8)\n    num_epochs: int = field(default=3)\n    learning_rate: float = field(default=5e-5)\n    device: str = field(default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Data loading\n    dataset: str = field(default=\"Skylion007/openwebtext\")\n    datasetconfig: Optional[str] = field(default=None)  # Configuration for the dataset if required\n    split: str = field(default=\"train\")\n    val_dataset: str = field(default=\"Skylion007/openwebtext\")\n    val_datasetconfig: Optional[str] = field(default=None)\n    val_split: str = field(default=\"validation\")\n\n    # Training output\n    save_dir: str = field(default=\"./saved_models\")\n\n    # Data type (optional, depends on your use case)\n    dtype: str = field(default=\"float32\")","metadata":{"execution":{"iopub.status.busy":"2023-11-20T23:59:10.034459Z","iopub.execute_input":"2023-11-20T23:59:10.034769Z","iopub.status.idle":"2023-11-20T23:59:10.074878Z","shell.execute_reply.started":"2023-11-20T23:59:10.034716Z","shell.execute_reply":"2023-11-20T23:59:10.072956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the main function\n\nThis is where the loop will go, and the model will be loaded and trained.","metadata":{}},{"cell_type":"code","source":"def main(args: TrainArgs):\n    try:\n        del model\n        \n    except:\n        pass\n    tokenizer = AutoTokenizer.from_pretrained(args.model)\n    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|USER|>\", \"<|ASSISTANT|>\"]})\n    train_data = load_dataset(args.dataset, args.datasetconfig, split=args.split)\n    val = load_dataset(args.val_dataset, args.val_datasetconfig, split=args.val_split)\n    dtype = STRING_TO_DTYPE_MAPPING.get(args.dtype)\n    # Uncomment this if you want to use wandb.\n    #wandb.init(dir=\"\", project=\"\")\n    train_data = ConversationDataset(tokenizer, data=train_data, max_length=256)\n    val_data = ConversationDataset(tokenizer, data=val, max_length=256)\n    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n    val_loader = DataLoader(val_data, batch_size=args.batch_size)\n\n    # Initialize the model\n    model = AutoModelForCausalLM.from_pretrained(args.model)\n    model.resize_token_embeddings(len(tokenizer))\n    model.to(args.device, dtype=dtype)\n    \n    # Define the optimizer\n    optimizer = optim.Adamax(model.parameters(), lr=args.learning_rate)\n    \n    # Training and evaluation loops\n    try:\n        for epoch in range(args.num_epochs):\n            print(f'Epoch: {epoch+1:02}')\n            train_loss = train(model, train_loader, optimizer, tokenizer, args.device)\n            val_metrics = evaluate(model, val_loader, tokenizer)\n\n            print(f'Epoch: {epoch+1:02}')\n            print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n            print(f'\\tValidation metrics: {val_metrics}')\n\n            # Calculate and display BLEU, accuracy, and any other desired metrics\n            # You'll need to implement this part based on your specific task\n    except KeyboardInterrupt:\n         print(\"Saving and cleaning up the model...\")\n         print(\"Do NOT kill the terminal it WILL corrupt the model files\")\n         model.save(args.save_dir)\n         quit(0)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T23:59:10.079511Z","iopub.execute_input":"2023-11-20T23:59:10.079939Z","iopub.status.idle":"2023-11-20T23:59:10.097475Z","shell.execute_reply.started":"2023-11-20T23:59:10.079894Z","shell.execute_reply":"2023-11-20T23:59:10.096462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = TrainArgs(\n    split=\"train[:100]\",\n    val_split=\"train[100:200]\",\n    dataset=\"Locutusque/InstructMix\",\n    val_dataset=\"Locutusque/InstructMix\"\n)\n    \nmain(args)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T00:07:16.562665Z","iopub.execute_input":"2023-11-21T00:07:16.563033Z","iopub.status.idle":"2023-11-21T00:08:19.985563Z","shell.execute_reply.started":"2023-11-21T00:07:16.563006Z","shell.execute_reply":"2023-11-21T00:08:19.983999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main loop\n\nThis is where we'll run the main function, and start the training process.","metadata":{}}]}