{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PyTorch training script\nThe following notebook will demonstrate how I trained TinyMistral.","metadata":{}},{"cell_type":"markdown","source":"# Installing libraries\nWe'll install any dependency that we need for this notebook.","metadata":{}},{"cell_type":"code","source":"# I'll also be installing one of my other packages for a model that I have private.\n# This package contains nice utilities that I'd rather not code again.\n!pip install --upgrade sentia datasets transformers evaluate rouge_score\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom evaluate import load as load_metric\nimport sacrebleu\nfrom tqdm import tqdm\nimport math\nfrom sentia import SENTIAForCausalLM\nimport wandb\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom typing import Optional\nfrom dataclasses import dataclass, field\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2023-11-21T07:47:56.989055Z","iopub.execute_input":"2023-11-21T07:47:56.989868Z","iopub.status.idle":"2023-11-21T07:48:45.238733Z","shell.execute_reply.started":"2023-11-21T07:47:56.989829Z","shell.execute_reply":"2023-11-21T07:48:45.237894Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentia\n  Obtaining dependency information for sentia from https://files.pythonhosted.org/packages/41/fa/123cb81e3daf5259bd3e4f4849ef8e69e1dac0673dc0d0b408fe04557fb6/sentia-1.15-py3-none-any.whl.metadata\n  Downloading sentia-1.15-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.0)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=2.0 in /opt/conda/lib/python3.10/site-packages (from sentia) (2.0.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentia) (3.2.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentia) (4.66.1)\nCollecting sacrebleu (from sentia)\n  Obtaining dependency information for sacrebleu from https://files.pythonhosted.org/packages/df/c0/ff53cb76c1b050ad25d056877ba6d3f6fa964134370c4ccf57ad933d6f72/sacrebleu-2.3.2-py3-none-any.whl.metadata\n  Downloading sacrebleu-2.3.2-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting rotary-embedding-torch (from sentia)\n  Obtaining dependency information for rotary-embedding-torch from https://files.pythonhosted.org/packages/75/24/4ee455d4a1ea6cb66e937187383c1d28abdf63e4895118a3cbe5c5347517/rotary_embedding_torch-0.3.5-py3-none-any.whl.metadata\n  Downloading rotary_embedding_torch-0.3.5-py3-none-any.whl.metadata (678 bytes)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from sentia) (0.16.0)\nCollecting configparser (from sentia)\n  Obtaining dependency information for configparser from https://files.pythonhosted.org/packages/81/a3/0e5ed11da4b7770c15f6f319abf053f46b5a06c7d4273c48469b7899bd89/configparser-6.0.0-py3-none-any.whl.metadata\n  Downloading configparser-6.0.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sentia) (5.9.3)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sentia) (0.4.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nCollecting pyarrow-hotfix (from datasets)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/36/9d/fed46a4d94d05bc400bdaeb02d277ca5e61965cebe25b6029990d2191c0b/pyarrow_hotfix-0.5-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.5-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nCollecting huggingface-hub>=0.18.0 (from datasets)\n  Obtaining dependency information for huggingface-hub>=0.18.0 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nINFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\nCollecting tokenizers<0.19,>=0.14 (from transformers)\n  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/eb/3d/eee5f3c572a3f4db2ebabf5bd4f284f356078a5b5d27e6229b4450d5c5e4/tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->sentia) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->sentia) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->sentia) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nCollecting einops>=0.3 (from rotary-embedding-torch->sentia)\n  Obtaining dependency information for einops>=0.3 from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nCollecting portalocker (from sacrebleu->sentia)\n  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->sentia) (0.9.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->sentia) (4.9.3)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (3.1.32)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (1.34.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (0.4.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->sentia) (3.20.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->sentia) (4.0.10)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0->sentia) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0->sentia) (1.3.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->sentia) (5.0.0)\nDownloading sentia-1.15-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading configparser-6.0.0-py3-none-any.whl (19 kB)\nDownloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\nDownloading rotary_embedding_torch-0.3.5-py3-none-any.whl (5.1 kB)\nDownloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=4112b891a6019fdf0a9d9df9df1f53f429e0d799676be8fdb0fb9d8b0f4bb55f\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: pyarrow-hotfix, portalocker, einops, configparser, sacrebleu, rouge_score, huggingface-hub, tokenizers, rotary-embedding-torch, transformers, datasets, sentia, evaluate\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.17.3\n    Uninstalling huggingface-hub-0.17.3:\n      Successfully uninstalled huggingface-hub-0.17.3\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.14.1\n    Uninstalling tokenizers-0.14.1:\n      Successfully uninstalled tokenizers-0.14.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.35.0\n    Uninstalling transformers-4.35.0:\n      Successfully uninstalled transformers-4.35.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed configparser-6.0.0 datasets-2.15.0 einops-0.7.0 evaluate-0.4.1 huggingface-hub-0.19.4 portalocker-2.8.2 pyarrow-hotfix-0.5 rotary-embedding-torch-0.3.5 rouge_score-0.1.2 sacrebleu-2.3.2 sentia-1.15 tokenizers-0.15.0 transformers-4.35.2\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# String to dtype mapping\n\nWe'll define a constant to provide an easy way to define dtypes on the command line. (This would be a bit more relevant if it was not a notebook)","metadata":{}},{"cell_type":"code","source":"STRING_TO_DTYPE_MAPPING = {\n    \"bfloat16\": torch.bfloat16,\n     \"float32\": torch.float32,\n     \"float16\": torch.float16,\n     \"float64\": torch.float64,\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T07:48:45.240258Z","iopub.execute_input":"2023-11-21T07:48:45.240565Z","iopub.status.idle":"2023-11-21T07:48:45.245283Z","shell.execute_reply.started":"2023-11-21T07:48:45.240536Z","shell.execute_reply":"2023-11-21T07:48:45.244195Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Dataset classes\n\nWe'll define the dataset classes that will be used for data preprocessing","metadata":{}},{"cell_type":"code","source":"class ConversationDataset(Dataset):\n    def __init__(self, tokenizer, max_length=512, data=None, device=\"cuda\"):\n        self.data = data\n        self.device = device\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        try:\n            # Most of the time I'll be using InstructMix for instruction-tuning\n            user = self.data[idx][\"Input\"]\n            assistant = self.data[idx][\"Output\"]\n        except KeyError:\n            # If I'm using MMLU for evaluation\n            user = self.data[idx][\"question\"]\n            ans_index = self.data[idx][\"answer\"]\n            assistant = self.data[idx][\"choices\"][ans_index]\n        \n        input_text = f\"<|USER|> {user} <|ASSISTANT|> {assistant} <|endoftext|>\"\n        target_text = f\"<|USER|> {user} <|ASSISTANT|> {assistant} <|endoftext|>\"\n        input_ids = self.tokenizer.encode(input_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        target_ids = self.tokenizer.encode(target_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n        target_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(target_ids))\n\n        return {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.int64, device=self.device),\n            \"labels\": torch.tensor(target_ids, dtype=torch.int64, device=self.device),\n        }\nclass CompletionDataset(Dataset):\n    def __init__(self, tokenizer, data, max_length=256, device=\"cuda\"):\n        self.data = data\n        self.device = device\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data[idx][\"text\"]\n        input_text = f\"{text} {self.tokenizer.eos_token}\"\n        target_text = f\"{text} {self.tokenizer.eos_token}\"\n        input_ids = self.tokenizer.encode(input_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        target_ids = self.tokenizer.encode(target_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n        target_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(target_ids))\n        return {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.int64, device=self.device),\n            \"labels\": torch.tensor(target_ids, dtype=torch.int64, device=self.device),\n        }","metadata":{"execution":{"iopub.status.busy":"2023-11-21T07:48:45.246830Z","iopub.execute_input":"2023-11-21T07:48:45.247170Z","iopub.status.idle":"2023-11-21T07:48:45.271618Z","shell.execute_reply.started":"2023-11-21T07:48:45.247138Z","shell.execute_reply":"2023-11-21T07:48:45.270511Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Training loop\n\nWe'll define the training loop, with a few metrics to keep track of the model's learning.","metadata":{}},{"cell_type":"code","source":"def train(model, dataloader, optimizer, tokenizer, device=\"cuda\"):\n    model.train()\n    model.to(device=device)\n    total_loss = 0\n    total_perplexity = 0\n\n    for i, batch in tqdm(enumerate(dataloader)):\n                input_ids = batch[\"input_ids\"].to(device)\n                target_ids = batch[\"labels\"].to(device)\n                # Generate the output and calculate the loss\n                outputs = model(input_ids=input_ids, labels=target_ids)\n                loss, logits = outputs[:2]\n                # Calculate the BLEU score\n                probs = F.softmax(logits, dim=-1)\n                predictions = torch.argmax(probs, dim=-1)\n                predictions_str = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions.tolist()]\n                target_ids_str = [tokenizer.decode(tgt, skip_special_tokens=True) for tgt in target_ids.tolist()]\n                print(predictions_str[0])\n                bleu_scores = []\n                accuracy_scores = []\n                for pred_str, target_str in zip(predictions_str, target_ids_str):\n                    bleu = sacrebleu.sentence_bleu(pred_str, [target_str])\n                    bleu_scores.append(bleu.score)\n                for pred_id, target_id in zip(predictions, target_ids):\n                    accuracy = SENTIAForCausalLM.calculate_accuracy(pred_id, target_id)\n                    accuracy_scores.append(accuracy)\n\n                accuracy = sum(accuracy_scores) / len(accuracy_scores)\n                bleu = sum(bleu_scores) / len(bleu_scores)\n                # Calculate the reward\n                # This reward can be used for RLHF, but I prefer using it as a metric.\n                # The highest value is typically around 2x the sequence length.\n                # The lowest value is typically about the negative of the sequence length.\n                reward, penalty = SENTIAForCausalLM.get_reward(predictions.tolist()[0], target_ids.tolist()[0], bleu)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                # Update the metrics\n                total_loss += loss.item()\n                try:\n                    wandb.log({\"loss\": ol.item(), \"bleu\": bleu, \"perplexity\": torch.exp(ol).item(), \"accuracy\": accuracy})\n                except:\n                    pass\n                print(\n                    f\"Batch {i + 1}/{len(dataloader)}: Loss - {loss.item():.4f}, NetReward - {reward - penalty:.4f}, BLEU - {bleu:.4f}, Perplexity - {torch.exp(loss).item()}, Accuracy - {accuracy}\")\n\n    return total_loss / len(dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T07:48:45.273801Z","iopub.execute_input":"2023-11-21T07:48:45.274072Z","iopub.status.idle":"2023-11-21T07:48:45.287970Z","shell.execute_reply.started":"2023-11-21T07:48:45.274047Z","shell.execute_reply":"2023-11-21T07:48:45.287054Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation loop\n\nWe'll define an evaluation loop with multiple metrics to track the model's performance. We'll include scores like loss, perplexity, bleu, rouge, and f1.","metadata":{}},{"cell_type":"code","source":"def evaluate(model, val_loader, tokenizer, use_cuda=True):\n    model.eval()\n    device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Load metrics\n    bleu_metric = load_metric('bleu')\n    rouge_metric = load_metric('rouge')\n    \n    # Initialize variables to accumulate scores\n    total_loss = 0\n    all_predictions = []\n    all_references = []\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n            # Move batch to the correct device\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            \n            # Forward pass\n            outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"])\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            # Convert logits to predictions (for F1, BLEU, ROUGE)\n            # This part depends on your model's output format and the task\n            # Here is a mock-up of how you might extract predictions\n            # For token classification tasks:\n            # predictions = outputs.logits.argmax(dim=-1)\n            # For seq2seq tasks:\n            predictions = tokenizer.batch_decode(outputs.logits.argmax(dim=-1), skip_special_tokens=True)\n\n            # Post-process batch to extract labels and predictions in a suitable format\n            references = batch['labels'] \n            references = tokenizer.batch_decode(references, skip_special_tokens=True)\n            \n            # Update metrics\n            references = [[ref] for ref in references]\n            bleu_metric.add_batch(predictions=predictions, references=references)\n            rouge_metric.add_batch(predictions=predictions, references=references)\n            # Store predictions and references for later use if needed\n            all_predictions.extend(predictions)\n            all_references.extend(references)\n    # Compute the metrics\n    bleu_score = bleu_metric.compute(predictions=all_predictions, references=all_references)\n    rouge_score = rouge_metric.compute(predictions=all_predictions, references=all_references)\n\n    # Perplexity can be calculated from the total loss\n    # For perplexity, we assume the loss is the negative log likelihood\n    # In case the loss function is something else, this needs to be adjusted\n    perplexity = torch.exp(torch.tensor(total_loss / len(val_loader)))\n\n    metrics = {\n        'val_loss': total_loss / len(val_loader),\n        'val_perplexity': perplexity.item(),\n        'val_bleu': bleu_score['bleu'],\n        'val_rouge': rouge_score,\n    }\n    try:\n        wandb.log(**metrics)\n    except:\n        pass\n\n    return metrics","metadata":{"execution":{"iopub.status.busy":"2023-11-21T07:48:45.289166Z","iopub.execute_input":"2023-11-21T07:48:45.289447Z","iopub.status.idle":"2023-11-21T07:48:45.303452Z","shell.execute_reply.started":"2023-11-21T07:48:45.289423Z","shell.execute_reply":"2023-11-21T07:48:45.302605Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# TrainArgs\n\nHere we'll define a class creates a config for the training.","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass TrainArgs:\n    # Model configuration\n    model: str = field(default=\"Locutusque/TinyMistral-248M\")  # Pretrained model name or path\n    batch_size: int = field(default=8)\n    num_epochs: int = field(default=3)\n    learning_rate: float = field(default=5e-5)\n    device: str = field(default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Data loading\n    dataset: str = field(default=\"Skylion007/openwebtext\")\n    datasetconfig: Optional[str] = field(default=None)  # Configuration for the dataset if required\n    split: str = field(default=\"train\")\n    val_dataset: str = field(default=\"Skylion007/openwebtext\")\n    val_datasetconfig: Optional[str] = field(default=None)\n    val_split: str = field(default=\"validation\")\n\n    # Training output\n    save_dir: str = field(default=\"./saved_models\")\n\n    # Data type (optional, depends on your use case)\n    dtype: str = field(default=\"float32\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T07:48:45.304486Z","iopub.execute_input":"2023-11-21T07:48:45.304725Z","iopub.status.idle":"2023-11-21T07:48:45.343036Z","shell.execute_reply.started":"2023-11-21T07:48:45.304704Z","shell.execute_reply":"2023-11-21T07:48:45.342228Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Define the main function\n\nThis is where the loop will go, and the model will be loaded and trained.","metadata":{}},{"cell_type":"code","source":"def main(args: TrainArgs):\n    try:\n        del model\n    except:\n        pass\n    tokenizer = AutoTokenizer.from_pretrained(args.model)\n    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|USER|>\", \"<|ASSISTANT|>\"]})\n    train_data = load_dataset(args.dataset, args.datasetconfig, split=args.split)\n    val = load_dataset(args.val_dataset, args.val_datasetconfig, split=args.val_split)\n    dtype = STRING_TO_DTYPE_MAPPING.get(args.dtype)\n    # Uncomment this if you want to use wandb.\n    #wandb.init(dir=\"\", project=\"\")\n    train_data = ConversationDataset(tokenizer, data=train_data, max_length=256)\n    val_data = ConversationDataset(tokenizer, data=val, max_length=256)\n    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n    val_loader = DataLoader(val_data, batch_size=args.batch_size)\n\n    # Initialize the model\n    model = AutoModelForCausalLM.from_pretrained(args.model)\n    model.resize_token_embeddings(len(tokenizer))\n    model.to(args.device, dtype=dtype)\n    \n    # Define the optimizer\n    optimizer = optim.Adamax(model.parameters(), lr=args.learning_rate)\n    \n    # Training and evaluation loops\n    try:\n        for epoch in range(args.num_epochs):\n            print(f'Epoch: {epoch+1:02}')\n            train_loss = train(model, train_loader, optimizer, tokenizer, args.device)\n            val_metrics = evaluate(model, val_loader, tokenizer)\n\n            print(f'Epoch: {epoch+1:02}')\n            print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n            print(f'\\tValidation metrics: {val_metrics}')\n            \n            model.save_pretrained(args.save_dir)\n\n            # Calculate and display BLEU, accuracy, and any other desired metrics\n            # You'll need to implement this part based on your specific task\n    except KeyboardInterrupt:\n         print(\"Saving and cleaning up the model...\")\n         print(\"Do NOT kill the terminal it WILL corrupt the model files\")\n         model.save_pretrained(args.save_dir)\n         quit(0)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T07:48:45.344200Z","iopub.execute_input":"2023-11-21T07:48:45.344497Z","iopub.status.idle":"2023-11-21T07:48:45.356619Z","shell.execute_reply.started":"2023-11-21T07:48:45.344473Z","shell.execute_reply":"2023-11-21T07:48:45.355694Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"args = TrainArgs(\n    split=\"train[100:50100]\",\n    val_split=\"train[:100]\",\n    dataset=\"Locutusque/InstructMix\",\n    val_dataset=\"Locutusque/InstructMix\",\n    model=\"Locutusque/TinyMistral-248M\",\n)\n    \nmain(args)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T07:48:45.357640Z","iopub.execute_input":"2023-11-21T07:48:45.357942Z","iopub.status.idle":"2023-11-21T07:50:02.575614Z","shell.execute_reply.started":"2023-11-21T07:48:45.357912Z","shell.execute_reply":"2023-11-21T07:50:02.574592Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"031c0531d79a487caa7364703f470a12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df44103e430742afb7ce6ac63aeb1f64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f48125644fe0432d933a41f4ecb496c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/67.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e55d5e0b534ca59f162f465128fe47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/565 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76973e098ed549dbafab7c4c266605dd"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976be2cbc81947d5b42d9ab84a658dd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/6.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f42af1b47a1d4b1ba4a1e20f407dc00c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/9.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baa44de934384fafa22969e0c8fe3577"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59713fd1da8148fca1350f10aac42097"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee2801e59554dc8a7c5e423f9715f35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceb16a1aed2143468f36636fe8275d1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e72a436f6a474c1e990e2850c3536fcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/562 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"002f221696c840d4a932be13bc1aca93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/992M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"472900483e384f2cbaf4e7c6b9bee3cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"577fc232449e4408bae05aa5439f4561"}},"metadata":{}},{"name":"stdout","text":"Epoch: 01\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"The1================ =\n { =2\n2\n","output_type":"stream"},{"name":"stderr","text":"1it [00:02,  2.88s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 1/13: Loss - 0.4687, NetReward - 259.5000, BLEU - 2.2019, Perplexity - 1.5979504585266113, Accuracy - 0.91552734375\nThe The\n","output_type":"stream"},{"name":"stderr","text":"2it [00:03,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 2/13: Loss - 0.4652, NetReward - 511.2500, BLEU - 8.6004, Perplexity - 1.5922991037368774, Accuracy - 0.92138671875\nThe1 = = = = = = = = =1\n =\n","output_type":"stream"},{"name":"stderr","text":"3it [00:04,  1.44s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 3/13: Loss - 0.8612, NetReward - 254.7500, BLEU - 3.5386, Perplexity - 2.3660149574279785, Accuracy - 0.8291015625\nThe\n\n","output_type":"stream"},{"name":"stderr","text":"4it [00:05,  1.28s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 4/13: Loss - 1.5200, NetReward - 256.2500, BLEU - 2.8414, Perplexity - 4.572220325469971, Accuracy - 0.646484375\nThe\n\n","output_type":"stream"},{"name":"stderr","text":"5it [00:06,  1.18s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 5/13: Loss - 1.2672, NetReward - 512.2500, BLEU - 5.6518, Perplexity - 3.5509769916534424, Accuracy - 0.7197265625\nThe1 = = = firstrusum\n = = =\n\n =\n","output_type":"stream"},{"name":"stderr","text":"6it [00:07,  1.13s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 6/13: Loss - 0.6357, NetReward - 515.2500, BLEU - 10.8327, Perplexity - 1.8884326219558716, Accuracy - 0.8583984375\nThe_\n","output_type":"stream"},{"name":"stderr","text":"7it [00:09,  1.10s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 7/13: Loss - 0.5141, NetReward - 512.2500, BLEU - 6.1759, Perplexity - 1.6721891164779663, Accuracy - 0.87646484375\nThe1emside,idays  .Cinger\n 1999 ; 1910 ,1\n1\n","output_type":"stream"},{"name":"stderr","text":"8it [00:10,  1.08s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 8/13: Loss - 0.5292, NetReward - 259.7500, BLEU - 3.3201, Perplexity - 1.6975147724151611, Accuracy - 0.8759765625\nThe1's The'ity\n .S. W.kner , 1912\n\n\n1\n","output_type":"stream"},{"name":"stderr","text":"9it [00:11,  1.06s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 9/13: Loss - 0.9589, NetReward - 515.7500, BLEU - 10.4390, Perplexity - 2.6087839603424072, Accuracy - 0.7548828125\nThe\n 1993, the firster familys - @- theen from and theothy waser was her store in  age of the building .  Woodtonum .\n was to workvise the school until and she the her her and and her a chance of needed. Sheothy was her mother wereated with the one of . The ling ofs  Love , The  Book @ ,ed book andathers the , The 1919 ,othy waser , of a heart attack , Sheer was a to attend her own , the of extent , her death's death . and she her booksg her family children wasoured into her. and she was not to find to for newint glass window . .  home's home . the. Marymund's birthday .b , , She\n\n\n","output_type":"stream"},{"name":"stderr","text":"10it [00:12,  1.05s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 10/13: Loss - 1.1402, NetReward - 286.7500, BLEU - 1.1967, Perplexity - 3.1274824142456055, Accuracy - 0.72314453125\nThe\n Pmitized of the\" by the Rock\"ms\" .S. . ,\n to the  same time as the as  19,1 to  1963.\nlying to be A\" , the  1863 , the dateathous \" of which , the   of the  , \"  the of the timelington Club been closed with the to theham , the thelivience to the of the of Staffdersance and and , Columbia , .\n is became the beginning of the uation of thenance . . the Rock , Arkansas the first of therendered to the Unitedancing forces government . the,arn .s  .peditionary  11, 1862 . The\n\n\n","output_type":"stream"},{"name":"stderr","text":"11it [00:13,  1.05s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 11/13: Loss - 1.2610, NetReward - 280.7500, BLEU - 2.4079, Perplexity - 3.5288262367248535, Accuracy - 0.6826171875\nThe\ner' in thecesterton in 11th 1979.  85\n. She years arrangements were held at the in the.rington,, one in Ster,s homeon .' She funeralhes were found in theorrington Church and . She 1999, she wasbur ,   of theemin , , 1999 , was the bookower My , in He\n\n\n","output_type":"stream"},{"name":"stderr","text":"12it [00:14,  1.05s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 12/13: Loss - 0.6822, NetReward - 523.0000, BLEU - 5.0314, Perplexity - 1.97823965549469, Accuracy - 0.79638671875\nThe\n   19th0s, theer was to work the was a her to her children. the herself on on her subjects. She and friends were her be toular activities non works . and she later not The\n\n\n","output_type":"stream"},{"name":"stderr","text":"13it [00:15,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 13/13: Loss - 0.6531, NetReward - 260.2500, BLEU - 2.3486, Perplexity - 1.9214311838150024, Accuracy - 0.8544921875\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2055f5dace094c87ba1b10559fede4b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bac17b8765f423cbd953f6aab3a0da8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f980a0873eda4d6d8f45e06b13f2418a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05ae878917ef42788f8a3f6c4a6b7b52"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|██████████| 13/13 [00:02<00:00,  5.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 01\n\tTrain Loss: 0.843 | Train PPL:   2.323\n\tValidation metrics: {'val_loss': 1.2361177939635057, 'val_perplexity': 3.4422242641448975, 'val_bleu': 0.037422328091953436, 'val_rouge': {'rouge1': 0.18577667792527455, 'rouge2': 0.035492463796036294, 'rougeL': 0.14399339390379115, 'rougeLsum': 0.142500923607717}}\nEpoch: 02\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"The\n = =\n hetic  = = \n \n","output_type":"stream"},{"name":"stderr","text":"1it [00:01,  1.11s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 1/13: Loss - 0.8132, NetReward - 515.7500, BLEU - 8.6692, Perplexity - 2.2552084922790527, Accuracy - 0.79052734375\nThe\n","output_type":"stream"},{"name":"stderr","text":"2it [00:02,  1.07s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 2/13: Loss - 0.5391, NetReward - 512.5000, BLEU - 11.1235, Perplexity - 1.7145174741744995, Accuracy - 0.84619140625\nThe\n = = = rusons = =  \n \n","output_type":"stream"},{"name":"stderr","text":"3it [00:03,  1.05s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 3/13: Loss - 0.7212, NetReward - 514.7500, BLEU - 9.6425, Perplexity - 2.0568747520446777, Accuracy - 0.78271484375\nThe\ner's ', in 1999\n but her  1960 , sheer was to the10rd Woodtonons to 10Ps , , ydon , She was her on to to .rington , andsex , and , andfriendathed her her husband andwin, , and her her 'or Maryss . She the the residence in St mother deter to deteriorate and She was the the out of hospital homes wasvalescent homes . and sheended to her . friends . She\n \n","output_type":"stream"},{"name":"stderr","text":"4it [00:04,  1.04s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 4/13: Loss - 0.6285, NetReward - 528.7500, BLEU - 9.6918, Perplexity - 1.874811053276062, Accuracy - 0.81103515625\nThe\n Pmities of the\" by the Rock\"ms\" .S. . ,  to the  same time as the as  19,1 to  1863.\nlying to the A\" , the  1863 , the dateathous \" of which for the   , April  , \"  the of the timelington Club been closed with sold to theham , the thelivience to the of the of Staffdersance . and of Columbia , The\n is became the beginning of the uation of thenance . . the Rock , Arkansas the first of therendered to the Unionancing forces government . the Douele .s  .pedition .  11 , 1863 . The\n \n","output_type":"stream"},{"name":"stderr","text":"5it [00:05,  1.04s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 5/13: Loss - 1.2145, NetReward - 538.2500, BLEU - 7.2919, Perplexity - 3.3685615062713623, Accuracy - 0.6513671875\nThe\n","output_type":"stream"},{"name":"stderr","text":"6it [00:06,  1.04s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 6/13: Loss - 0.8619, NetReward - 512.5000, BLEU - 7.9106, Perplexity - 2.367687940597534, Accuracy - 0.7451171875\nThe0\nols,1aired) for the) the)\n \n","output_type":"stream"},{"name":"stderr","text":"7it [00:07,  1.05s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 7/13: Loss - 0.6135, NetReward - 258.5000, BLEU - 4.1453, Perplexity - 1.846954584121704, Accuracy - 0.8076171875\n\n","output_type":"stream"},{"name":"stderr","text":"8it [00:08,  1.05s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 8/13: Loss - 0.6292, NetReward - 512.5000, BLEU - 8.4909, Perplexity - 1.8760173320770264, Accuracy - 0.78173828125\n\n","output_type":"stream"},{"name":"stderr","text":"9it [00:09,  1.05s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 9/13: Loss - 0.3821, NetReward - 512.5000, BLEU - 7.5634, Perplexity - 1.4653677940368652, Accuracy - 0.8837890625\n\n","output_type":"stream"},{"name":"stderr","text":"10it [00:10,  1.04s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 10/13: Loss - 0.4757, NetReward - 256.5000, BLEU - 3.5535, Perplexity - 1.609196424484253, Accuracy - 0.87255859375\n\nform  \n \n","output_type":"stream"},{"name":"stderr","text":"11it [00:11,  1.04s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 11/13: Loss - 0.5558, NetReward - 513.0000, BLEU - 7.4118, Perplexity - 1.7434208393096924, Accuracy - 0.802734375\n\n","output_type":"stream"},{"name":"stderr","text":"12it [00:12,  1.04s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 12/13: Loss - 0.7422, NetReward - 256.5000, BLEU - 3.6220, Perplexity - 2.100522994995117, Accuracy - 0.755859375\n\n --@-ewaelite are a grouphold andelong friend on theer .\n was said that \"I am not be extent a by the . I by the way sense, but in a way of style matter . the way of the of have have.\" .\n also that fewness for the \"  of the Lett,igan , theThe early and she and the Gs , @@ , \"\n \n","output_type":"stream"},{"name":"stderr","text":"13it [00:13,  1.03s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 13/13: Loss - 0.4044, NetReward - 270.7500, BLEU - 1.8706, Perplexity - 1.4983537197113037, Accuracy - 0.9013671875\n","output_type":"stream"},{"name":"stderr","text":"\nEvaluating: 100%|██████████| 13/13 [00:02<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 02\n\tTrain Loss: 0.660 | Train PPL:   1.935\n\tValidation metrics: {'val_loss': 1.202731552032324, 'val_perplexity': 3.3291985988616943, 'val_bleu': 0.037862437295002736, 'val_rouge': {'rouge1': 0.1924113636236676, 'rouge2': 0.0357157964800078, 'rougeL': 0.14648140597457493, 'rougeLsum': 0.14643715086711367}}\nEpoch: 03\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n: a Updatedued  \n \n","output_type":"stream"},{"name":"stderr","text":"1it [00:01,  1.06s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 1/13: Loss - 0.7341, NetReward - 257.2500, BLEU - 4.9300, Perplexity - 2.083533763885498, Accuracy - 0.78125\n\nLOely Mary Anner\n110 June 1893 – 29 February 1963) was an English poetator and known for her series of illustr novelsations depicting theies and fair in Sheer wass workworks was in hood and the from in workshops in the ageydon School of Art . She work work work was theeting cards and avenile drawings coversations , and her first book, Theower Fairies , the  , was published in 1911 . She works were published in   year . \n \n","output_type":"stream"},{"name":"stderr","text":"2it [00:02,  1.04s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 2/13: Loss - 0.7058, NetReward - 533.7500, BLEU - 8.2474, Perplexity - 2.0255537033081055, Accuracy - 0.73046875\n er was asally in the and and a and-@ and @-@ and. but she was also wellent at painting and-@ and @-@ . . and herils , and in herel . She also a largebook , her own theuring the and .  also wrote that \"I' a been to paint aively , my way that I naturally to me . but any doubt effort or effort . detail expression . She  \n","output_type":"stream"},{"name":"stderr","text":"3it [00:03,  1.03s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 3/13: Loss - 1.0852, NetReward - 528.0000, BLEU - 6.3949, Perplexity - 2.96004581451416, Accuracy - 0.65234375\n\n","output_type":"stream"},{"name":"stderr","text":"4it [00:04,  1.03s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 4/13: Loss - 0.3484, NetReward - 512.5000, BLEU - 11.0016, Perplexity - 1.4168387651443481, Accuracy - 0.87646484375\n =er was in thething Hospital in 19 September 1963. aged 89 years . She years arrangements were held at the in the.rington and , one in Ster's Churchon . , She motherhes were found in theorrington Church , .  1919 , she wasne , a parish of theenguin Books , 1990 , was the bookower Fairy of .   \n","output_type":"stream"},{"name":"stderr","text":"5it [00:05,  1.02s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 5/13: Loss - 0.5118, NetReward - 530.0000, BLEU - 5.4033, Perplexity - 1.6682847738265991, Accuracy - 0.81689453125\n = Maway  born  friend and a inspiration on the mother . Sheer wass worklike included aalgic clothing and wellaway's children . , and sheer 's children are often popularancholy and more like- appearance . and to to the in technology technology . Sheer ' the in her interestical eye and a a to her .s childrenator , who Warrant ,  with herwich , sheator and B. Williamsward , designed theer .s work .   \n","output_type":"stream"},{"name":"stderr","text":"6it [00:06,  1.03s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 6/13: Loss - 0.6772, NetReward - 527.7500, BLEU - 5.6826, Perplexity - 1.9684313535690308, Accuracy - 0.75732421875\n =er wass etches and  , and illustr , her . taken to children and family friends children of children children in andated to theities causes , char organizations events , and toited in the artworks . The also the covers , booksyets , and other a of bookscards . theaphael anduck and the artistsers . as the of Bookss the Worldies ,11919 ) and andasonside Booksiday , 1916 ) , and The's The and Girl (acters ( 1918 ) 1910 ) .\n work works Fymes ( the the ( 1920 ) .  Great of the Rings ( ( ( 1929 ) were  collection of a woman who was in theans and a farmbank , and publishedically ac received by  in 1800 , thesw , herighlaceaces ,11910 ) were the a family who Mary who wasues a father from a and a use of the \"y of  book of a illustr friendotsoge Mc-@-- who \". and , , , hisally illustr that youngensian version experiment .  and Dan was\n","output_type":"stream"},{"name":"stderr","text":"7it [00:07,  1.03s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 7/13: Loss - 0.8725, NetReward - 295.0000, BLEU - 3.6722, Perplexity - 2.3928308486938477, Accuracy - 0.6962890625\n\n","output_type":"stream"},{"name":"stderr","text":"8it [00:08,  1.03s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 8/13: Loss - 0.4458, NetReward - 512.5000, BLEU - 8.6532, Perplexity - 1.5617531538009644, Accuracy - 0.84326171875\n ='s The andacters  . W. Waulkner , 1912 \n \n","output_type":"stream"},{"name":"stderr","text":"9it [00:09,  1.02s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 9/13: Loss - 0.3334, NetReward - 516.5000, BLEU - 8.4207, Perplexity - 1.3957371711730957, Accuracy - 0.8583984375\n =aired  painted by \n \n","output_type":"stream"},{"name":"stderr","text":"10it [00:10,  1.03s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 10/13: Loss - 0.3323, NetReward - 514.2500, BLEU - 9.5667, Perplexity - 1.3941354751586914, Accuracy - 0.88427734375\n\n","output_type":"stream"},{"name":"stderr","text":"11it [00:11,  1.03s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 11/13: Loss - 0.4473, NetReward - 512.5000, BLEU - 7.2057, Perplexity - 1.5641283988952637, Accuracy - 0.83203125\n =er was a from in the and literature in  1930 . She 1919 , the19 , , she published the art class at the Universityydon School of Art . where began the school . the 1920s . She  , she became a teaching position at \n \n","output_type":"stream"},{"name":"stderr","text":"12it [00:12,  1.03s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 12/13: Loss - 0.3252, NetReward - 524.7500, BLEU - 10.5997, Perplexity - 1.3842436075210571, Accuracy - 0.88671875\n = = =  B-@ ed\n = =  \n \n","output_type":"stream"},{"name":"stderr","text":"13it [00:13,  1.01s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 13/13: Loss - 0.8529, NetReward - 514.7500, BLEU - 15.6588, Perplexity - 2.346405267715454, Accuracy - 0.703125\n","output_type":"stream"},{"name":"stderr","text":"\nEvaluating: 100%|██████████| 13/13 [00:02<00:00,  6.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 03\n\tTrain Loss: 0.590 | Train PPL:   1.804\n\tValidation metrics: {'val_loss': 1.1854317142413213, 'val_perplexity': 3.272099256515503, 'val_bleu': 0.040601190480815126, 'val_rouge': {'rouge1': 0.1936679557912107, 'rouge2': 0.03818811964951879, 'rougeL': 0.1489430670434423, 'rougeLsum': 0.14944877531720746}}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Main loop\n\nThis is where we'll run the main function, and start the training process.","metadata":{}}]}