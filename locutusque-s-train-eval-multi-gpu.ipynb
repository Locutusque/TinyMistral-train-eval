{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7007410,"sourceType":"datasetVersion","datasetId":4028499}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PyTorch Training Script with Hugging Face, Accelerate, and DeepSpeed\n\nThis notebook provides a parameterized training loop using PyTorch. It demonstrates how to load models and datasets from Hugging Face, distribute the model across multiple GPUs with Accelerate and DeepSpeed, and includes a custom dataset class that tokenizes examples simultaneously with the training loop.","metadata":{}},{"cell_type":"code","source":"# We'll need to install and setup accelerate and deepspeed configs for this notebook.\n# I'll also be installing one of my other packages for a model that I have private.\n# This package contains nice utilities that I'd rather not code again.\n\n!pip install --upgrade deepspeed accelerate sentia evaluate datasets transformers rouge_score\n!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n!python -m pip install -i https://pypi.anaconda.org/mpi4py/simple mpi4py\n!sudo apt install -y openmpi-bin\n!sudo apt install -y mpich\n!pip install --upgrade deepspeed accelerate sentia evaluate datasets transformers rouge_score\n!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n!python -m pip install -i https://pypi.anaconda.org/mpi4py/simple mpi4py\n!accelerate config","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:56:27.000113Z","iopub.execute_input":"2023-11-20T16:56:27.001170Z","iopub.status.idle":"2023-11-20T16:58:56.047918Z","shell.execute_reply.started":"2023-11-20T16:56:27.001130Z","shell.execute_reply":"2023-11-20T16:58:56.046586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary modules\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nfrom torch.optim import AdamW\nimport torch.nn.functional as F\nfrom sentia import SENTIAForCausalLM # This the model I was talking about,\n# it includes methods for accuracy calculation\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom accelerate import Accelerator\nimport accelerate\nimport wandb\nfrom tqdm import tqdm\nimport sacrebleu\nfrom datasets import load_dataset\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Tuple\nfrom evaluate import load as load_metric\nimport warnings","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:00.326786Z","iopub.execute_input":"2023-11-20T16:59:00.327201Z","iopub.status.idle":"2023-11-20T16:59:25.864874Z","shell.execute_reply.started":"2023-11-20T16:59:00.327139Z","shell.execute_reply":"2023-11-20T16:59:25.863635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom Dataset Classes\n\nThese classes are designed to tokenize data on-the-fly, which can be more memory-efficient for large datasets.","metadata":{}},{"cell_type":"code","source":"class ConversationDataset(Dataset):\n    def __init__(self, tokenizer, max_length=512, data=None, device=\"cuda\"):\n        self.data = data\n        self.device = device\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        try:\n            # Most of the time I'll be using InstructMix for instruction-tuning\n            user = self.data[idx][\"instruction\"]\n            input = self.data[idx][\"input\"] or \"\"\n            assistant = self.data[idx][\"output\"]\n        except KeyError:\n            # If I'm using MMLU for evaluation\n            user = self.data[idx][\"question\"]\n            ans_index = self.data[idx][\"answer\"]\n            assistant = self.data[idx][\"choices\"][ans_index]\n        \n        input_text = f\"<|USER|> {user} {input} <|ASSISTANT|> {assistant} <|endoftext|>\"\n        target_text = f\"<|USER|> {user} {input} <|ASSISTANT|> {assistant} <|endoftext|>\"\n        input_ids = self.tokenizer.encode(input_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        target_ids = self.tokenizer.encode(target_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n        target_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(target_ids))\n\n        return {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.int64, device=self.device),\n            \"labels\": torch.tensor(target_ids, dtype=torch.int64, device=self.device),\n        }\nclass CompletionDataset(Dataset):\n    def __init__(self, tokenizer, data, max_length=256, device=\"cuda\"):\n        self.data = data\n        self.device = device\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data[idx][\"text\"]\n        input_text = f\"{text} {self.tokenizer.eos_token}\"\n        target_text = f\"{text} {self.tokenizer.eos_token}\"\n        input_ids = self.tokenizer.encode(input_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        target_ids = self.tokenizer.encode(target_text, add_special_tokens=True, max_length=self.max_length, truncation=True)\n        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n        target_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(target_ids))\n        return {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.int64, device=self.device),\n            \"labels\": torch.tensor(target_ids, dtype=torch.int64, device=self.device),\n        }","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:25.889718Z","iopub.execute_input":"2023-11-20T16:59:25.890112Z","iopub.status.idle":"2023-11-20T16:59:25.912341Z","shell.execute_reply.started":"2023-11-20T16:59:25.890079Z","shell.execute_reply":"2023-11-20T16:59:25.911116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training args\nWe'll define some training arguments, these will be able to control the parameters of training.\n","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass TrainingArguments:\n    # Model and tokenizer arguments\n    model_name_or_path: str\n    tokenizer_name: Optional[str] = None\n    \n    # Training data arguments\n    train_data_file: str = None\n    eval_data_file: str = None\n    train_data_config: Optional[str] = None\n    eval_data_config: Optional[str] = None\n    max_seq_length: int = 512\n    \n    # Training procedure arguments\n    num_train_epochs: int = 3\n    train_batch_size: int = 8\n    eval_batch_size: int = 8\n    learning_rate: float = 5e-5\n    weight_decay: float = 0.01\n    adam_epsilon: float = 1e-8\n    max_grad_norm: float = 1.0\n    gradient_accumulation_steps: int = 1\n    \n    # Logging, saving, and evaluation arguments\n    print_predictions: bool = False\n    logging_steps: int = 50\n    output_dir: str = \"./output\"\n    \n    # DeepSpeed configuration\n    deepspeed_config_file: Tuple[dict, str] = None\n    \n    # Accelerate configuration\n    mixed_precision: str = \"no\"  # Options: \"no\", \"fp16\", \"bf16\"\n    \n    # WandB configuration\n    use_wandb: bool = False\n    wandb_project: Optional[str] = None\n    wandb_entity: Optional[str] = None\n    \n    # Other arguments\n    seed: int = 42\n    device: str = \"cuda\"  # Options: \"cuda\", \"cpu\"\n    local_rank: int = -1  # For distributed training: local_rank for distributed training on gpus\n\n    def __post_init__(self):\n        if self.tokenizer_name is None:\n            self.tokenizer_name = self.model_name_or_path\n        if self.local_rank == -1:\n            # If not using distributed training, set mixed precision\n            if self.mixed_precision == \"fp16\":\n                self.fp16 = True\n            elif self.mixed_precision == \"bf16\":\n                self.bf16 = True\n            else:\n                self.fp16 = False\n                self.bf16 = False\n        else:\n            # For distributed training, disable mixed precision here\n            # and let deepspeed handle it\n            self.fp16 = False\n            self.bf16 = False\n        if self.use_wandb and (self.wandb_project is None):\n            raise ValueError(\"wandb project name must be defined if use_wandb = True\")","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:25.915291Z","iopub.execute_input":"2023-11-20T16:59:25.915746Z","iopub.status.idle":"2023-11-20T16:59:25.933409Z","shell.execute_reply.started":"2023-11-20T16:59:25.915707Z","shell.execute_reply":"2023-11-20T16:59:25.932440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training and Evaluation Functions\n\nThese functions define the training and evaluation loops for the model. They use the `Accelerator` class for GPU acceleration.","metadata":{}},{"cell_type":"code","source":"def train(model, dataloader, optimizer, tokenizer, args: TrainingArguments, accelerator: Accelerator):\n    model.train()\n    total_loss = 0\n    total_perplexity = 0\n    global_step = 0\n\n    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n        with accelerator.accumulate():\n            # Move batch to the correct device\n            batch = {k: v.to(args.device) for k, v in batch.items()}\n            input_ids = batch[\"input_ids\"]\n            labels = batch[\"labels\"]\n\n            # Generate the output and calculate the loss\n            outputs = model(input_ids=input_ids, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n            # Backward pass\n            accelerator.backward(loss)\n            if args.max_grad_norm is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n            # Update model parameters\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Calculate the BLEU score and accuracy\n            predictions = torch.argmax(logits, dim=-1)\n            predictions_str = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions.tolist()]\n            target_ids_str = [tokenizer.decode(tgt, skip_special_tokens=True) for tgt in batch[\"labels\"].tolist()]\n            bleu_scores = []\n            accuracy_scores = []\n            for pred_str, target_str in zip(predictions_str, target_ids_str):\n                bleu = sacrebleu.sentence_bleu(pred_str, [target_str])\n                bleu_scores.append(bleu.score)\n\n            bleu = sum(bleu_scores) / len(bleu_scores)\n\n            # Logging\n            try:\n                wandb.log({\n                        \"loss\": loss.item(),\n                        \"bleu\": bleu,\n                        \"perplexity\": torch.exp(loss).item(),\n                    })\n            except Exception as e:\n                warnings.warn(f\"An error occurred while logging to Weights & Biases: {e}\")\n\n            # Print training information\n            if global_step % args.logging_steps == 0:\n                print(f\"Step {global_step}: Loss: {loss.item():.4f}, BLEU: {bleu:.4f}, Perplexity: {torch.exp(loss).item():.4f}\")\n            \n\n            # Update the metrics\n            total_loss += loss.item()\n            total_perplexity += torch.exp(loss).item()\n\n    return total_loss / len(dataloader), total_perplexity / len(dataloader)\n\ndef evaluate(model, val_loader, tokenizer, use_cuda=True):\n    model.eval()\n    device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Load metrics\n    f1_metric = load_metric('f1')\n    bleu_metric = load_metric('bleu')\n    rouge_metric = load_metric('rouge')\n    \n    # Initialize variables to accumulate scores\n    total_loss = 0\n    all_predictions = []\n    all_references = []\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n            # Move batch to the correct device\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n            \n            # Forward pass\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            # Convert logits to predictions (for F1, BLEU, ROUGE)\n            # This part depends on your model's output format and the task\n            # Here is a mock-up of how you might extract predictions\n            # For token classification tasks:\n            # predictions = outputs.logits.argmax(dim=-1)\n            # For seq2seq tasks:\n            predictions = tokenizer.batch_decode(outputs.logits.argmax(dim=-1), skip_special_tokens=True)\n\n            # Post-process batch to extract labels and predictions in a suitable format\n            references = batch['labels'] \n            references = tokenizer.batch_decode(references, skip_special_tokens=True)\n            \n            # Update metrics\n            f1_metric.add_batch(predictions=predictions, references=references)\n            bleu_metric.add_batch(predictions=[predictions], references=[[references]])\n            rouge_metric.add_batch(predictions=predictions, references=references)\n            # Store predictions and references for later use if needed\n            all_predictions.extend(predictions)\n            all_references.extend(references)\n    # Compute the metrics\n    f1_score = f1_metric.compute(predictions=all_predictions, references=all_references, average='macro')\n    bleu_score = bleu_metric.compute(predictions=[all_predictions], references=[[all_references]])\n    rouge_score = rouge_metric.compute(predictions=all_predictions, references=all_references)\n\n    # Perplexity can be calculated from the total loss\n    # For perplexity, we assume the loss is the negative log likelihood\n    # In case the loss function is something else, this needs to be adjusted\n    perplexity = torch.exp(torch.tensor(total_loss / len(val_loader)))\n\n    metrics = {\n        'val_loss': total_loss / len(val_loader),\n        'val_perplexity': perplexity.item(),\n        'val_f1': f1_score['f1'],\n        'val_bleu': bleu_score['bleu'],\n        'val_rouge': rouge_score,\n    }\n    try:\n        wandb.log(**metrics)\n    except:\n        pass\n\n    return metrics\n\n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:25.934869Z","iopub.execute_input":"2023-11-20T16:59:25.935213Z","iopub.status.idle":"2023-11-20T16:59:25.963767Z","shell.execute_reply.started":"2023-11-20T16:59:25.935169Z","shell.execute_reply":"2023-11-20T16:59:25.962740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop\nHere, we'll load the model, datasets, and tokenizer and start the training loop.","metadata":{}},{"cell_type":"code","source":"def main():\n    !export CUDA_VISIBLE_DEVICES=0,1\n    deepspeed_config_dict = {\n        \"train_micro_batch_size_per_gpu\": 4,\n        \"gradient_accumulation_steps\": 32,\n        \"fp16\": {\n            \"enabled\": True,\n            \"loss_scale\": 0,\n            \"loss_scale_window\": 1000,\n            \"min_loss_scale\": 1,\n            \"hysteresis\": 2\n        },\n        \"zero_optimization\": {\n            \"stage\": 2,\n            \"offload_optimizer\": {\n                \"device\": \"cpu\",\n                \"pin_memory\": True\n            },\n            \"allgather_partitions\": True,\n            \"allgather_bucket_size\": 2e8,\n            \"overlap_comm\": True,\n            \"reduce_scatter\": True,\n            \"reduce_bucket_size\": 2e8,\n            \"contiguous_gradients\": True\n        },\n        \"activation_checkpointing\": {\n            \"partition_activations\": True,\n            \"cpu_checkpointing\": False,\n            \"contiguous_memory_optimization\": False,\n            \"synchronize_checkpoint_boundary\": False\n        },\n        \"steps_per_print\": 1,\n        \"wall_clock_breakdown\": False\n    }\n    deepspeed_config_dict = accelerate.utils.DeepSpeedPlugin(hf_ds_config=deepspeed_config_dict)\n    args = TrainingArguments(\n        model_name_or_path=\"Locutusque/TinyMistral-248M\",\n        train_data_file=\"tatsu-lab/alpaca\",\n        eval_data_file=\"cais/mmlu\",\n        eval_data_config=\"all\",\n        max_seq_length=256,\n        max_grad_norm=2.0,\n        train_batch_size=4,\n        eval_batch_size=4,\n        gradient_accumulation_steps=32,\n        adam_epsilon=1e-4,\n        use_wandb=False,\n        print_predictions=False,\n        deepspeed_config_file=deepspeed_config_dict,\n\n    )\n    use_wandb = args.use_wandb\n    # Initialize Weights & Biases if you're using it\n    if use_wandb:\n        wandb.init(project=args.wandb_project, entity=args.wandb_entity, settings=wandb.Settings(start_method=\"fork\"))\n\n    # Initialize the Accelerator and tokenizer\n    print(\"Installing the tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name if args.tokenizer_name is not None else args.model_name_or_path)\n    print(\"Initializing the accelerator\")\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, deepspeed_plugin=args.deepspeed_config_file)\n    print(accelerator.state.num_processes)\n\n    # Prepare the dataset and dataloader\n    print(\"Installing the datasets\")\n    train_data = load_dataset(args.train_data_file, args.train_data_config, split=\"train[:100]\")\n    val_data = load_dataset(args.eval_data_file, args.eval_data_config, split=\"validation\")\n    train_dataset = ConversationDataset(tokenizer=tokenizer, data=train_data)\n    val_dataset = ConversationDataset(tokenizer=tokenizer, data=val_data)\n    train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=args.eval_batch_size)\n\n    # Prepare the model and optimizer\n    print(\"Installing the model\")\n    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, torch_dtype=torch.float16).to(\"cuda:0\")\n    optimizer = AdamW(model.parameters(), lr=args.learning_rate, fused=True, eps=args.adam_epsilon)\n\n    # Prepare the model, optimizer, and dataloaders for distributed training\n    model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, val_dataloader\n    )\n\n    # Training loop\n    num_epochs = 3\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        train_loss = train(model, train_dataloader, optimizer, tokenizer, args=args, accelerator=accelerator)\n        if args.output_dir is not None:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir)\n        metrics = evaluate(model, val_dataloader, tokenizer, use_cuda=True)\n        print(f\"Training Loss: {train_loss}\")\n        print(f\"Validation Metrics: {metrics}\")\n\n    # Finalize Weights & Biases run\n    if use_wandb:\n        wandb.finish(quiet=True)\n\n    print(\"Training complete!\")\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T16:59:48.641248Z","iopub.execute_input":"2023-11-20T16:59:48.642632Z","iopub.status.idle":"2023-11-20T16:59:56.743794Z","shell.execute_reply.started":"2023-11-20T16:59:48.642541Z","shell.execute_reply":"2023-11-20T16:59:56.737452Z"},"trusted":true},"execution_count":null,"outputs":[]}]}